{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "sns.set_theme(style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Putting Together a Base Dataset\n",
    "\n",
    "In this notebook we are going to create a base table on which to train a first simple model on. This dataset will contain \n",
    "- __Cell-line features__ like Gene Expression and Copy-Number-Variation information as one input and \n",
    "- __Drug Features__ as the de-Morgan fingerprints for specific drugs as another input. \n",
    "\n",
    "These features got created in the following notebooks:  \n",
    "- `02_GDSC_map_GenExpr.ipynb`\n",
    "- `03_GDSC_map_CNV.ipynb`\n",
    "- `05_DrugFeatures.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cwoest/Documents/Academics/Data_Science_UP/master_thesis/material/GNN-material\n",
      "37700889        0 drwxr-xr-x    8 cwoest           staff                 256 May  9 16:27 ../../datasets/gdsc/my_datasets\n",
      "37938781    96424 -rw-r--r--    1 cwoest           staff            49365274 May  9 15:54 ../../datasets/gdsc/my_datasets/gdsc_base.pkl\n",
      "37704542  7461488 -rw-r--r--    1 cwoest           staff            3820277941 May  5 14:52 ../../datasets/gdsc/my_datasets/joined_gdsc_cnv_gistic.pkl\n",
      "37701975  6435440 -rw-r--r--    1 cwoest           staff            3292923584 May  9 10:59 ../../datasets/gdsc/my_datasets/joined_gdsc_geneexpr.pkl\n",
      "37867797    14480 -rw-r--r--    1 cwoest           staff             7413076 May  9 11:31 ../../datasets/gdsc/my_datasets/geneexpr_sparse.pkl\n",
      "37940951     1504 -rw-r--r--    1 cwoest           staff              769884 May  9 16:27 ../../datasets/gdsc/my_datasets/drug_name_fingerprints.pkl\n",
      "37700990  7681336 -rw-r--r--    1 cwoest           staff            3932841931 May  5 13:42 ../../datasets/gdsc/my_datasets/joined_gdsc_cnv_picnic.pkl\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!find ../../datasets/gdsc/my_datasets -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVE_DATA_TO = '../../datasets/gdsc/my_datasets/'\n",
    "\n",
    "# GDSC base table.\n",
    "GDSC_BASE_FILE = 'gdsc_base.pkl'\n",
    "\n",
    "# Cell-line feature\n",
    "CNV_GISTIC_FILE = 'joined_gdsc_cnv_gistic.pkl'\n",
    "CNV_PICNIC_FILE = 'joined_gdsc_cnv_picnic.pkl'\n",
    "GENE_EXPR_FILE = 'joined_gdsc_geneexpr.pkl'\n",
    "\n",
    "# Drug features\n",
    "FINGERPRINTS_FILE = 'drug_name_fingerprints.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download\n",
    "\n",
    "Download data created by the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File `gdsc_base.pkl` took 0.07293 seconds to import. \n",
      "Shape: (446521, 14)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gdsc_base = pd.read_pickle(f'{PATH_TO_SAVE_DATA_TO}{GDSC_BASE_FILE}')\n",
    "print(f\"File `{GDSC_BASE_FILE}` took {time.time()-start:.5f} seconds to import. \\nShape: {gdsc_base.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File `joined_gdsc_cnv_gistic.pkl` took 73.98715 seconds to import. \n",
      "Shape: (446521, 952)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cnv_gistic = pd.read_pickle(f'{PATH_TO_SAVE_DATA_TO}{CNV_GISTIC_FILE}')\n",
    "print(f\"File `{CNV_GISTIC_FILE}` took {time.time()-start:.5f} seconds to import. \\nShape: {cnv_gistic.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File `joined_gdsc_cnv_picnic.pkl` took 80.10469 seconds to import. \n",
      "Shape: (446521, 980)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cnv_picnic = pd.read_pickle(f'{PATH_TO_SAVE_DATA_TO}{CNV_PICNIC_FILE}')\n",
    "print(f\"File `{CNV_PICNIC_FILE}` took {time.time()-start:.5f} seconds to import. \\nShape: {cnv_picnic.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File `joined_gdsc_geneexpr.pkl` took 4.24449 seconds to import. \n",
      "Shape: (446521, 922)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gene_expr = pd.read_pickle(f'{PATH_TO_SAVE_DATA_TO}{GENE_EXPR_FILE}')\n",
    "print(f\"File `{GENE_EXPR_FILE}` took {time.time()-start:.5f} seconds to import. \\nShape: {gene_expr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File `drug_name_fingerprints.pkl` took 0.05682 seconds to import. \n",
      "Length: 449\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "fingerprints = pd.read_pickle(f'{PATH_TO_SAVE_DATA_TO}{FINGERPRINTS_FILE}')\n",
    "print(f\"File `{FINGERPRINTS_FILE}` took {time.time()-start:.5f} seconds to import. \\nLength: {len(fingerprints)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    GDSC base    : (446521, 14)\n",
      "    CNV Gistic   : (446521, 952)\n",
      "    CNV Picnic   : (446521, 980)\n",
      "    Gene Expr    : (446521, 922)\n",
      "    Fingerprints : 449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "    GDSC base    : {gdsc_base.shape}\n",
    "    CNV Gistic   : {cnv_gistic.shape}\n",
    "    CNV Picnic   : {cnv_picnic.shape}\n",
    "    Gene Expr    : {gene_expr.shape}\n",
    "    Fingerprints : {len(fingerprints)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Simple Model - Gene Expression Only\n",
    "\n",
    "First, we try to create a simple Neural Network using the gene expression information only. Meaning, we will only use table `gene_expr`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DRUG_ID</th>\n",
       "      <th>CELL_LINE_NAME</th>\n",
       "      <th>AUC</th>\n",
       "      <th>CELL_ID</th>\n",
       "      <th>LN_IC50</th>\n",
       "      <th>CONC</th>\n",
       "      <th>MASTER_CELL_ID</th>\n",
       "      <th>INTENSITY</th>\n",
       "      <th>DATASET</th>\n",
       "      <th>Z_SCORE</th>\n",
       "      <th>...</th>\n",
       "      <th>MYCBP</th>\n",
       "      <th>FIS1</th>\n",
       "      <th>IFRD2</th>\n",
       "      <th>NPEPL1</th>\n",
       "      <th>CEBPD</th>\n",
       "      <th>PLEKHM1</th>\n",
       "      <th>MIF</th>\n",
       "      <th>PRAF2</th>\n",
       "      <th>LYN</th>\n",
       "      <th>POLG2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MC-CAR</td>\n",
       "      <td>0.982114</td>\n",
       "      <td>3137</td>\n",
       "      <td>2.395685</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49</td>\n",
       "      <td>544404</td>\n",
       "      <td>GDSC1</td>\n",
       "      <td>-0.189576</td>\n",
       "      <td>...</td>\n",
       "      <td>8.355826</td>\n",
       "      <td>8.951680</td>\n",
       "      <td>7.205590</td>\n",
       "      <td>3.277948</td>\n",
       "      <td>3.465672</td>\n",
       "      <td>6.312806</td>\n",
       "      <td>12.112498</td>\n",
       "      <td>3.010237</td>\n",
       "      <td>8.750848</td>\n",
       "      <td>6.199366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>ES3</td>\n",
       "      <td>0.984816</td>\n",
       "      <td>2366</td>\n",
       "      <td>3.140923</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1342</td>\n",
       "      <td>404197</td>\n",
       "      <td>GDSC1</td>\n",
       "      <td>0.508635</td>\n",
       "      <td>...</td>\n",
       "      <td>5.995760</td>\n",
       "      <td>9.337588</td>\n",
       "      <td>7.468226</td>\n",
       "      <td>3.716270</td>\n",
       "      <td>5.363887</td>\n",
       "      <td>6.188079</td>\n",
       "      <td>12.281947</td>\n",
       "      <td>4.794624</td>\n",
       "      <td>3.588528</td>\n",
       "      <td>6.785201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>ES5</td>\n",
       "      <td>0.985693</td>\n",
       "      <td>2368</td>\n",
       "      <td>3.968757</td>\n",
       "      <td>2.0</td>\n",
       "      <td>610</td>\n",
       "      <td>797378</td>\n",
       "      <td>GDSC1</td>\n",
       "      <td>1.284229</td>\n",
       "      <td>...</td>\n",
       "      <td>6.939741</td>\n",
       "      <td>8.688176</td>\n",
       "      <td>7.085349</td>\n",
       "      <td>3.688222</td>\n",
       "      <td>4.572119</td>\n",
       "      <td>6.345090</td>\n",
       "      <td>12.276166</td>\n",
       "      <td>4.114092</td>\n",
       "      <td>5.768098</td>\n",
       "      <td>7.505155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 922 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    DRUG_ID CELL_LINE_NAME       AUC  CELL_ID   LN_IC50  CONC  MASTER_CELL_ID  \\\n",
       "0         1         MC-CAR  0.982114     3137  2.395685   2.0              49   \n",
       "9         1            ES3  0.984816     2366  3.140923   2.0            1342   \n",
       "27        1            ES5  0.985693     2368  3.968757   2.0             610   \n",
       "\n",
       "    INTENSITY DATASET   Z_SCORE  ...     MYCBP      FIS1     IFRD2    NPEPL1  \\\n",
       "0      544404   GDSC1 -0.189576  ...  8.355826  8.951680  7.205590  3.277948   \n",
       "9      404197   GDSC1  0.508635  ...  5.995760  9.337588  7.468226  3.716270   \n",
       "27     797378   GDSC1  1.284229  ...  6.939741  8.688176  7.085349  3.688222   \n",
       "\n",
       "       CEBPD   PLEKHM1        MIF     PRAF2       LYN     POLG2  \n",
       "0   3.465672  6.312806  12.112498  3.010237  8.750848  6.199366  \n",
       "9   5.363887  6.188079  12.281947  4.794624  3.588528  6.785201  \n",
       "27  4.572119  6.345090  12.276166  4.114092  5.768098  7.505155  \n",
       "\n",
       "[3 rows x 922 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_expr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN_IC50</th>\n",
       "      <th>CONC</th>\n",
       "      <th>INTENSITY</th>\n",
       "      <th>TSPAN6</th>\n",
       "      <th>SCYL3</th>\n",
       "      <th>BAD</th>\n",
       "      <th>LAP3</th>\n",
       "      <th>SNX11</th>\n",
       "      <th>CASP10</th>\n",
       "      <th>CFLAR</th>\n",
       "      <th>...</th>\n",
       "      <th>MYCBP</th>\n",
       "      <th>FIS1</th>\n",
       "      <th>IFRD2</th>\n",
       "      <th>NPEPL1</th>\n",
       "      <th>CEBPD</th>\n",
       "      <th>PLEKHM1</th>\n",
       "      <th>MIF</th>\n",
       "      <th>PRAF2</th>\n",
       "      <th>LYN</th>\n",
       "      <th>POLG2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.395685</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544404</td>\n",
       "      <td>3.238273</td>\n",
       "      <td>4.856061</td>\n",
       "      <td>5.900525</td>\n",
       "      <td>8.120975</td>\n",
       "      <td>6.789716</td>\n",
       "      <td>3.593983</td>\n",
       "      <td>6.747933</td>\n",
       "      <td>...</td>\n",
       "      <td>8.355826</td>\n",
       "      <td>8.951680</td>\n",
       "      <td>7.205590</td>\n",
       "      <td>3.277948</td>\n",
       "      <td>3.465672</td>\n",
       "      <td>6.312806</td>\n",
       "      <td>12.112498</td>\n",
       "      <td>3.010237</td>\n",
       "      <td>8.750848</td>\n",
       "      <td>6.199366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.140923</td>\n",
       "      <td>2.0</td>\n",
       "      <td>404197</td>\n",
       "      <td>8.690198</td>\n",
       "      <td>4.572198</td>\n",
       "      <td>6.927127</td>\n",
       "      <td>5.595564</td>\n",
       "      <td>5.040800</td>\n",
       "      <td>2.776345</td>\n",
       "      <td>4.813174</td>\n",
       "      <td>...</td>\n",
       "      <td>5.995760</td>\n",
       "      <td>9.337588</td>\n",
       "      <td>7.468226</td>\n",
       "      <td>3.716270</td>\n",
       "      <td>5.363887</td>\n",
       "      <td>6.188079</td>\n",
       "      <td>12.281947</td>\n",
       "      <td>4.794624</td>\n",
       "      <td>3.588528</td>\n",
       "      <td>6.785201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.968757</td>\n",
       "      <td>2.0</td>\n",
       "      <td>797378</td>\n",
       "      <td>8.233101</td>\n",
       "      <td>4.749715</td>\n",
       "      <td>7.123143</td>\n",
       "      <td>5.458094</td>\n",
       "      <td>4.598347</td>\n",
       "      <td>2.900356</td>\n",
       "      <td>5.130654</td>\n",
       "      <td>...</td>\n",
       "      <td>6.939741</td>\n",
       "      <td>8.688176</td>\n",
       "      <td>7.085349</td>\n",
       "      <td>3.688222</td>\n",
       "      <td>4.572119</td>\n",
       "      <td>6.345090</td>\n",
       "      <td>12.276166</td>\n",
       "      <td>4.114092</td>\n",
       "      <td>5.768098</td>\n",
       "      <td>7.505155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 911 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LN_IC50  CONC  INTENSITY    TSPAN6     SCYL3       BAD      LAP3  \\\n",
       "0   2.395685   2.0     544404  3.238273  4.856061  5.900525  8.120975   \n",
       "9   3.140923   2.0     404197  8.690198  4.572198  6.927127  5.595564   \n",
       "27  3.968757   2.0     797378  8.233101  4.749715  7.123143  5.458094   \n",
       "\n",
       "       SNX11    CASP10     CFLAR  ...     MYCBP      FIS1     IFRD2    NPEPL1  \\\n",
       "0   6.789716  3.593983  6.747933  ...  8.355826  8.951680  7.205590  3.277948   \n",
       "9   5.040800  2.776345  4.813174  ...  5.995760  9.337588  7.468226  3.716270   \n",
       "27  4.598347  2.900356  5.130654  ...  6.939741  8.688176  7.085349  3.688222   \n",
       "\n",
       "       CEBPD   PLEKHM1        MIF     PRAF2       LYN     POLG2  \n",
       "0   3.465672  6.312806  12.112498  3.010237  8.750848  6.199366  \n",
       "9   5.363887  6.188079  12.281947  4.794624  3.588528  6.785201  \n",
       "27  4.572119  6.345090  12.276166  4.114092  5.768098  7.505155  \n",
       "\n",
       "[3 rows x 911 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_expr_base = gene_expr.loc[:, ~gene_expr.columns.isin(\n",
    "    ['DRUG_ID', \n",
    "    'CELL_LINE_NAME',\n",
    "    'CELL_ID',\n",
    "    'MASTER_CELL_ID',\n",
    "    'DATASET',\n",
    "    'Z_SCORE',\n",
    "    'DRUG_NAME',\n",
    "    'RMSE',\n",
    "    'AUC',\n",
    "    'COSMIC_ID',\n",
    "    'POSITION'])\n",
    "]\n",
    "gene_expr_base.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cwoest/Documents/Academics/Data_Science_UP/master_thesis/material/GNN-material/06_create_base_dataset.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cwoest/Documents/Academics/Data_Science_UP/master_thesis/material/GNN-material/06_create_base_dataset.ipynb#ch0000055?line=0'>1</a>\u001b[0m v \u001b[39m=\u001b[39m gene_expr\u001b[39m.\u001b[39;49mloc[:, gene_expr\u001b[39m.\u001b[39;49mcolumns\u001b[39m!=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLN_IC50\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues\n",
      "File \u001b[0;32m/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/frame.py:10883\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/frame.py?line=10809'>10810</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m  <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/frame.py?line=10810'>10811</a>\u001b[0m \u001b[39mReturn a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[1;32m  <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/frame.py?line=10811'>10812</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/frame.py?line=10879'>10880</a>\u001b[0m \u001b[39m       ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[1;32m  <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/frame.py?line=10880'>10881</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m  <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/frame.py?line=10881'>10882</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m> <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/frame.py?line=10882'>10883</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mas_array()\n",
      "File \u001b[0;32m/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py:1589\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1586'>1587</a>\u001b[0m             arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1587'>1588</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1588'>1589</a>\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interleave(dtype\u001b[39m=\u001b[39;49mdtype, na_value\u001b[39m=\u001b[39;49mna_value)\n\u001b[1;32m   <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1589'>1590</a>\u001b[0m     \u001b[39m# The underlying data was copied within _interleave\u001b[39;00m\n\u001b[1;32m   <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1590'>1591</a>\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py:1638\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1635'>1636</a>\u001b[0m         arr \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mget_values(dtype)\n\u001b[1;32m   <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1636'>1637</a>\u001b[0m         result[rl\u001b[39m.\u001b[39mindexer] \u001b[39m=\u001b[39m arr\n\u001b[0;32m-> <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1637'>1638</a>\u001b[0m         itemmask[rl\u001b[39m.\u001b[39mindexer] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1638'>1639</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m   <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/pandas/core/internals/managers.py?line=1640'>1641</a>\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "v = gene_expr.loc[:, gene_expr.columns!='LN_IC50'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = gene_expr.loc[:, ~gene_expr.columns.isin(['LN_IC50'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'LN_IC50' in gene_expr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = gene_expr['LN_IC50'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20435448"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_expr_base.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 446521 entries, 0 to 5707264\n",
      "Columns: 911 entries, LN_IC50 to POLG2\n",
      "dtypes: float64(910), int64(1)\n",
      "memory usage: 3.0 GB\n"
     ]
    }
   ],
   "source": [
    "gene_expr_base.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446521, 911)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_expr_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(424015, 911)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_nonnan = gene_expr_base.dropna()\n",
    "gene_nonnan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.00000000e+00, 5.44404000e+05, 3.23827251e+00, ...,\n",
       "        3.01023733e+00, 8.75084800e+00, 6.19936581e+00],\n",
       "       [2.00000000e+00, 4.04197000e+05, 8.69019791e+00, ...,\n",
       "        4.79462383e+00, 3.58852758e+00, 6.78520131e+00],\n",
       "       [2.00000000e+00, 7.97378000e+05, 8.23310113e+00, ...,\n",
       "        4.11409177e+00, 5.76809818e+00, 7.50515490e+00],\n",
       "       ...,\n",
       "       [1.00000000e+01, 1.59580000e+04, 8.59362481e+00, ...,\n",
       "        3.34734214e+00, 7.21468659e+00, 6.58715604e+00],\n",
       "       [1.00000000e+01, 2.54190000e+04, 8.44162845e+00, ...,\n",
       "        3.44324413e+00, 8.34896465e+00, 6.33635721e+00],\n",
       "       [1.00000000e+01, 1.34730000e+04, 6.98164324e+00, ...,\n",
       "        4.30818433e+00, 2.91918023e+00, 6.40410310e+00]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_expr_base.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 910 # which is gene_expr_base.shape[0]-1 = 911 - 1\n",
    "                 # One value per feature\n",
    "hidden_size = 100\n",
    "num_classes = 1\n",
    "num_epochs = 2\n",
    "batch_size = 10_000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pytorch Dataset out of the gene expression dataframe.\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    \"\"\"Customized class for Gene Expression data preparation.\"\"\"\n",
    "\n",
    "    def __init__(self, pd_geneexpr_dataframe: pd.DataFrame, target_name: str = 'LN_IC50'):\n",
    "        \"\"\"Initializes the dataset object.\"\"\"\n",
    "        gene_expr_df = pd_geneexpr_dataframe\n",
    "        assert target_name in gene_expr_df.columns\n",
    "\n",
    "        X = gene_expr_df.loc[:, ~gene_expr_df.columns.isin([target_name])].values\n",
    "        y = gene_expr_df[target_name].values\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads and returns a sample from the dataset at the given index `idx`.\"\"\"\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_expr_dataset = GeneExpressionDataset(pd_geneexpr_dataframe=gene_expr_base,\n",
    "                                          target_name='LN_IC50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_split_ratio = 0.8\n",
    "\n",
    "train_size = int(train_set_split_ratio * len(gene_expr_dataset))\n",
    "test_size = len(gene_expr_dataset) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(gene_expr_dataset, \n",
    "                                                    [train_size, test_size],\n",
    "                                                    generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Shapes \n",
      "        Train : \n",
      "            X : torch.Size([446521, 910])\n",
      "            y : torch.Size([446521])\n",
      "        Test  :\n",
      "            X : torch.Size([446521, 910])\n",
      "            y : torch.Size([446521])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "    Shapes \n",
    "        Train : \n",
    "            X : {train_set.dataset.X.shape}\n",
    "            y : {train_set.dataset.y.shape}\n",
    "        Test  :\n",
    "            X : {test_set.dataset.X.shape}\n",
    "            y : {test_set.dataset.y.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    i = 0\n",
      "        X = tensor([[1.0240e+01, 8.7761e+05, 7.8985e+00,  ..., 4.1138e+00, 3.1903e+00,\n",
      "         5.4369e+00],\n",
      "        [2.0000e+01, 4.3556e+05, 3.3886e+00,  ..., 4.6263e+00, 6.8246e+00,\n",
      "         6.4895e+00],\n",
      "        [1.0000e+01, 4.3293e+04, 8.2143e+00,  ..., 4.1281e+00, 6.0327e+00,\n",
      "         7.5018e+00],\n",
      "        ...,\n",
      "        [5.0000e+01, 2.3497e+07, 3.1896e+00,  ..., 3.8713e+00, 9.2537e+00,\n",
      "         5.6479e+00],\n",
      "        [2.0000e+00, 2.4176e+04, 3.6547e+00,  ..., 4.6862e+00, 5.5265e+00,\n",
      "         5.0838e+00],\n",
      "        [1.0000e+01, 9.5980e+03, 8.8486e+00,  ..., 3.4825e+00, 7.0952e+00,\n",
      "         5.9602e+00]])\n",
      "        y = tensor([5.7684, 3.7713, 6.1509,  ..., 0.9570, 3.1641, 1.3138])\n",
      "        X.size : torch.Size([10000, 910])\n",
      "        y.size : torch.Size([10000])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for i, (X, y) in enumerate(train_loader): \n",
    "    if i == 1: break\n",
    "    print(f\"\"\"\n",
    "    i = {i}\n",
    "        X = {X}\n",
    "        y = {y}\n",
    "        X.size : {X.size()}\n",
    "        y.size : {y.size()}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count : 36\n",
      "test count  : 9\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, (X, y) in enumerate(train_loader): \n",
    "    count += 1\n",
    "print(f\"train count : {count}\") \n",
    "\n",
    "count = 0\n",
    "for i, (X, y) in enumerate(test_loader): \n",
    "    count += 1\n",
    "print(f\"test count  : {count}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 910 # which is gene_expr_base.shape[0]-1 = 911 - 1\n",
    "                 # One value per feature\n",
    "hidden_size = 100\n",
    "num_classes = 1\n",
    "num_epochs = 2\n",
    "batch_size = 10_000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class SimpleNN_GeneExpr(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN_GeneExpr, self).__init__()\n",
    "        print(f\"\"\"\n",
    "            input_size  = {input_size}\n",
    "            hidden_size = {hidden_size}\n",
    "            num_classes = {num_classes}\n",
    "        \"\"\")\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            input_size  = 910\n",
      "            hidden_size = 100\n",
      "            num_classes = 1\n",
      "        \n",
      "SimpleNN_GeneExpr(\n",
      "  (fc1): Linear(in_features=910, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN_GeneExpr(input_size, hidden_size, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([10000])) that is different to the input size (torch.Size([10000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [5/36], Loss: nan\n",
      "Epoch [1/2], Step [10/36], Loss: nan\n",
      "Epoch [1/2], Step [15/36], Loss: nan\n",
      "Epoch [1/2], Step [20/36], Loss: nan\n",
      "Epoch [1/2], Step [25/36], Loss: nan\n",
      "Epoch [1/2], Step [30/36], Loss: nan\n",
      "Epoch [1/2], Step [35/36], Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([7216])) that is different to the input size (torch.Size([7216, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [5/36], Loss: nan\n",
      "Epoch [2/2], Step [10/36], Loss: nan\n",
      "Epoch [2/2], Step [15/36], Loss: nan\n",
      "Epoch [2/2], Step [20/36], Loss: nan\n",
      "Epoch [2/2], Step [25/36], Loss: nan\n",
      "Epoch [2/2], Step [30/36], Loss: nan\n",
      "Epoch [2/2], Step [35/36], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "it = iter(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):  \n",
    "        #images = images.reshape(-1, 28*28).to(device)\n",
    "        #labels = labels.to(device)\n",
    "\n",
    "        # Move tensors to the configured device\n",
    "        X = X_batch.to(device)  # All feature values\n",
    "        y = y_batch.to(device)  # The ln(IC50) values\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        print(outputs)\n",
    "        if i == 0: break\n",
    "        loss = loss_func(outputs, y_batch)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 5 == 0: # was % 100 before\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------\n",
    "# Define Hyperparameterspace\n",
    "# --------------------------------- \n",
    "input_size = 910 # which is gene_expr_base.shape[0]-1 = 911 - 1\n",
    "                 # One value per feature\n",
    "hidden_size = 100\n",
    "num_classes = 1\n",
    "num_epochs = 10\n",
    "batch_size = 20_000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# ---------------------------------\n",
    "# Create the Dataset\n",
    "# ---------------------------------\n",
    "gene_expr_dataset = GeneExpressionDataset(pd_geneexpr_dataframe=gene_nonnan,\n",
    "                                          target_name='LN_IC50')\n",
    "\n",
    "train_set_split_ratio = 0.8\n",
    "\n",
    "train_size = int(train_set_split_ratio * len(gene_expr_dataset))\n",
    "test_size = len(gene_expr_dataset) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(gene_expr_dataset, \n",
    "                                                    [train_size, test_size],\n",
    "                                                    generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            input_size  = 910\n",
      "            hidden_size = 64\n",
      "            num_classes = 1\n",
      "        \n",
      "SimpleNN_GeneExpr(\n",
      "  (fc1): Linear(in_features=910, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Model structure: SimpleNN_GeneExpr(\n",
      "  (fc1): Linear(in_features=910, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Layer: fc1.weight | Size: torch.Size([64, 910]) | Values : tensor([[-0.0269, -0.0289,  0.0310,  ...,  0.0093, -0.0131, -0.0231],\n",
      "        [-0.0095, -0.0036,  0.0024,  ..., -0.0028,  0.0158,  0.0255]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc1.bias | Size: torch.Size([64]) | Values : tensor([-0.0276, -0.0186], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc2.weight | Size: torch.Size([1, 64]) | Values : tensor([[-0.0814,  0.0776, -0.0997, -0.0004, -0.1216,  0.1208,  0.1022, -0.0368,\n",
      "         -0.0604, -0.0807, -0.0704,  0.0839,  0.0568,  0.0574,  0.0327,  0.0534,\n",
      "          0.0700, -0.0401,  0.0793,  0.1118,  0.0104,  0.0471, -0.0391,  0.0346,\n",
      "          0.0680,  0.0139, -0.1200,  0.0823,  0.1050,  0.0068, -0.0195,  0.0454,\n",
      "          0.0035, -0.1032,  0.0451, -0.0736, -0.0667,  0.0112,  0.0479, -0.0230,\n",
      "         -0.0661, -0.0828, -0.0018,  0.0727, -0.0174, -0.1138, -0.0363, -0.1210,\n",
      "         -0.0104,  0.1035, -0.0100,  0.1181,  0.0741,  0.0301,  0.0850,  0.0719,\n",
      "          0.0352, -0.0438, -0.0049,  0.0545, -0.1025, -0.0191, -0.0996, -0.0271]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc2.bias | Size: torch.Size([1]) | Values : tensor([0.1219], grad_fn=<SliceBackward0>) \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([20000])) that is different to the input size (torch.Size([20000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [5/17], Loss: 1679014272.0000\n",
      "Epoch [1/10], Step [10/17], Loss: 1081321856.0000\n",
      "Epoch [1/10], Step [15/17], Loss: 593561472.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([19212])) that is different to the input size (torch.Size([19212, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [5/17], Loss: 333701728.0000\n",
      "Epoch [2/10], Step [10/17], Loss: 161830144.0000\n",
      "Epoch [2/10], Step [15/17], Loss: 38565292.0000\n",
      "Epoch [3/10], Step [5/17], Loss: 54327852.0000\n",
      "Epoch [3/10], Step [10/17], Loss: 21425388.0000\n",
      "Epoch [3/10], Step [15/17], Loss: 6576365.5000\n",
      "Epoch [4/10], Step [5/17], Loss: 10743684.0000\n",
      "Epoch [4/10], Step [10/17], Loss: 6531094.5000\n",
      "Epoch [4/10], Step [15/17], Loss: 3904108.5000\n",
      "Epoch [5/10], Step [5/17], Loss: 313738.0938\n",
      "Epoch [5/10], Step [10/17], Loss: 142707.8750\n",
      "Epoch [5/10], Step [15/17], Loss: 31234.7090\n",
      "Epoch [6/10], Step [5/17], Loss: 342373.9375\n",
      "Epoch [6/10], Step [10/17], Loss: 200063.3906\n",
      "Epoch [6/10], Step [15/17], Loss: 95424.7031\n",
      "Epoch [7/10], Step [5/17], Loss: 22191.3223\n",
      "Epoch [7/10], Step [10/17], Loss: 30197.8672\n",
      "Epoch [7/10], Step [15/17], Loss: 21614.7305\n",
      "Epoch [8/10], Step [5/17], Loss: 2963.5806\n",
      "Epoch [8/10], Step [10/17], Loss: 5392.8320\n",
      "Epoch [8/10], Step [15/17], Loss: 3439.2063\n",
      "Epoch [9/10], Step [5/17], Loss: 1120.5652\n",
      "Epoch [9/10], Step [10/17], Loss: 1125.1642\n",
      "Epoch [9/10], Step [15/17], Loss: 322.1948\n",
      "Epoch [10/10], Step [5/17], Loss: 336.6836\n",
      "Epoch [10/10], Step [10/17], Loss: 115.4442\n",
      "Epoch [10/10], Step [15/17], Loss: 61.7933\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# Build the Neural Netowrk\n",
    "# ---------------------------------\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class SimpleNN_GeneExpr(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN_GeneExpr, self).__init__()\n",
    "        print(f\"\"\"\n",
    "            input_size  = {input_size}\n",
    "            hidden_size = {hidden_size}\n",
    "            num_classes = {num_classes}\n",
    "        \"\"\")\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)   # module that applies a linear transformation on the input using its stored weights and biases.\n",
    "        self.relu = nn.ReLU()                           # Non-linear activations are what create the complex mappings between the model’s inputs and outputs. No introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN_GeneExpr(input_size, hidden_size=64, num_classes=num_classes).to(device)\n",
    "print(model)\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "print(100*\"-\")\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# ---------------------------------\n",
    "# Train the model\n",
    "# ---------------------------------\n",
    "loss_values = []\n",
    "total_step = len(train_loader)\n",
    "it = iter(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):  \n",
    "        #images = images.reshape(-1, 28*28).to(device)\n",
    "        #labels = labels.to(device)\n",
    "        # if i > 3: break\n",
    "\n",
    "        # Move tensors to the configured device\n",
    "        X = X_batch.to(device)  # All feature values\n",
    "        y = y_batch.to(device)  # The ln(IC50) values\n",
    "        # print(20*\"+\")\n",
    "        # print(f\"i={i}\")\n",
    "        # print(X.size())\n",
    "        # print(y.size())\n",
    "        # print(X)\n",
    "        # print(y)\n",
    "        assert not torch.isnan(X).any(), \"X has NaN in it\"\n",
    "        assert not torch.isnan(X).any(), \"y has NaN in it\"\n",
    "        # print(\"outputs...\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = loss_func(outputs, y_batch)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 5 == 0: # was % 100 before\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))    \n",
    "\n",
    "    loss_values.append(running_loss / len(gene_expr_dataset))                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_values, linestyle='-', marker='o', label='Train');\n",
    "plt.xlim(0, 9, 1);\n",
    "plt.xlabel(\"Epochs\");\n",
    "plt.ylabel(\"MSE\");\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight tensor([[-0.0269, -0.0289,  0.0310,  ...,  0.0093, -0.0131, -0.0231],\n",
      "        [-0.0155, -0.0096, -0.0036,  ..., -0.0088,  0.0098,  0.0195],\n",
      "        [ 0.0061, -0.0083,  0.0073,  ..., -0.0298,  0.0049,  0.0245],\n",
      "        ...,\n",
      "        [-0.0231,  0.0236,  0.0253,  ..., -0.0141,  0.0007, -0.0226],\n",
      "        [-0.0111, -0.0113,  0.0097,  ..., -0.0209, -0.0089,  0.0112],\n",
      "        [-0.0198,  0.0234,  0.0064,  ..., -0.0039, -0.0163,  0.0033]])\n",
      "fc1.bias tensor([-0.0276, -0.0246, -0.0266, -0.0269, -0.0033, -0.0299, -0.0013, -0.0185,\n",
      "         0.0138, -0.0015,  0.0075, -0.0261, -0.0012, -0.0195,  0.0197,  0.0119,\n",
      "        -0.0245,  0.0285,  0.0317,  0.0275, -0.0239, -0.0060, -0.0016,  0.0263,\n",
      "         0.0138, -0.0275,  0.0059, -0.0246, -0.0264, -0.0240,  0.0010,  0.0252,\n",
      "        -0.0282, -0.0232, -0.0184,  0.0026, -0.0216, -0.0088,  0.0056, -0.0270,\n",
      "        -0.0378,  0.0226,  0.0226,  0.0072,  0.0023, -0.0321,  0.0131, -0.0111,\n",
      "        -0.0102, -0.0136, -0.0045,  0.0121, -0.0047, -0.0238,  0.0271, -0.0172,\n",
      "        -0.0134, -0.0103, -0.0109,  0.0037, -0.0090,  0.0075, -0.0068,  0.0204])\n",
      "fc2.weight tensor([[-8.1411e-02,  7.1644e-02, -9.9706e-02,  2.6776e-05, -1.1649e-01,\n",
      "          1.1832e-01,  1.0222e-01, -3.2355e-02, -5.5286e-02, -8.0706e-02,\n",
      "         -6.9941e-02,  8.0809e-02,  5.6776e-02,  5.5773e-02,  3.2670e-02,\n",
      "          4.4120e-02,  6.8061e-02, -4.0212e-02,  7.7981e-02,  1.0996e-01,\n",
      "          8.5480e-03,  4.5282e-02, -3.6753e-02,  3.1841e-02,  6.2050e-02,\n",
      "          1.3931e-02, -1.1917e-01,  7.6720e-02,  9.9224e-02,  5.0782e-03,\n",
      "         -1.8708e-02,  4.3240e-02, -2.4851e-03, -1.0368e-01,  4.5143e-02,\n",
      "         -7.4234e-02, -6.4498e-02,  1.1221e-02,  4.6230e-02, -2.3577e-02,\n",
      "         -6.1004e-02, -8.3285e-02, -1.7648e-03,  6.6641e-02, -1.7389e-02,\n",
      "         -1.1385e-01, -3.6293e-02, -1.2159e-01, -4.9431e-03,  9.7645e-02,\n",
      "         -1.0349e-02,  1.1655e-01,  7.2179e-02,  3.0129e-02,  7.8993e-02,\n",
      "          5.5161e-02,  3.5170e-02, -4.3846e-02,  2.0443e-04,  4.8420e-02,\n",
      "         -9.9522e-02, -1.9701e-02, -9.4085e-02, -2.7696e-02]])\n",
      "fc2.bias tensor([0.1211])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424015"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset.dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 910])\n",
      "tensor([5.0000e+00, 1.6405e+05, 7.7230e+00, 3.8891e+00, 7.0276e+00, 7.1788e+00,\n",
      "        4.5978e+00, 2.9991e+00, 5.7122e+00, 7.3056e+00, 5.0119e+00, 5.5608e+00,\n",
      "        5.0372e+00, 5.9888e+00, 8.3537e+00, 8.2142e+00, 3.4273e+00, 3.1605e+00,\n",
      "        6.7381e+00, 7.3724e+00, 7.4720e+00, 7.4916e+00, 6.9882e+00, 3.4057e+00,\n",
      "        8.5154e+00, 3.5651e+00, 4.2408e+00, 8.2399e+00, 6.6031e+00, 2.6824e+00,\n",
      "        3.8723e+00, 6.0092e+00, 4.2084e+00, 6.1314e+00, 7.8983e+00, 9.1007e+00,\n",
      "        3.8978e+00, 4.6455e+00, 8.1857e+00, 6.1570e+00, 7.6178e+00, 6.9075e+00,\n",
      "        8.0815e+00, 7.8934e+00, 9.4280e+00, 7.3091e+00, 4.6702e+00, 1.0381e+01,\n",
      "        8.1890e+00, 8.1509e+00, 9.9711e+00, 6.3114e+00, 6.4095e+00, 2.6861e+00,\n",
      "        8.6869e+00, 7.0460e+00, 6.1478e+00, 8.4304e+00, 5.5530e+00, 8.5280e+00,\n",
      "        5.5426e+00, 9.4177e+00, 4.8715e+00, 6.3504e+00, 5.3794e+00, 8.6218e+00,\n",
      "        4.4241e+00, 5.5954e+00, 3.7659e+00, 3.4134e+00, 6.3883e+00, 7.5156e+00,\n",
      "        5.4214e+00, 8.4898e+00, 7.8857e+00, 3.2003e+00, 4.5581e+00, 7.1813e+00,\n",
      "        5.9231e+00, 7.7191e+00, 2.9549e+00, 1.0078e+01, 6.3074e+00, 6.9804e+00,\n",
      "        2.9996e+00, 8.4429e+00, 5.3472e+00, 3.1104e+00, 7.6139e+00, 9.2105e+00,\n",
      "        4.2274e+00, 1.0879e+01, 9.4286e+00, 6.1927e+00, 6.0900e+00, 6.9827e+00,\n",
      "        5.5683e+00, 7.2076e+00, 5.4091e+00, 8.5944e+00, 4.2756e+00, 7.5467e+00,\n",
      "        3.5817e+00, 7.6517e+00, 4.4826e+00, 4.3378e+00, 7.3372e+00, 7.6342e+00,\n",
      "        3.2728e+00, 3.0777e+00, 4.8526e+00, 9.0984e+00, 1.0803e+01, 5.8230e+00,\n",
      "        4.7859e+00, 3.4712e+00, 6.2547e+00, 2.7402e+00, 8.2403e+00, 3.1455e+00,\n",
      "        8.3721e+00, 7.0272e+00, 3.2611e+00, 3.2007e+00, 8.7213e+00, 3.7587e+00,\n",
      "        3.9687e+00, 1.0051e+01, 8.8123e+00, 7.7698e+00, 4.5997e+00, 4.0361e+00,\n",
      "        9.8034e+00, 6.7992e+00, 8.0520e+00, 8.2579e+00, 7.8106e+00, 7.7218e+00,\n",
      "        4.1833e+00, 3.5080e+00, 7.3817e+00, 5.6421e+00, 2.9968e+00, 2.7528e+00,\n",
      "        7.8373e+00, 9.4042e+00, 7.3153e+00, 9.2995e+00, 5.4508e+00, 3.9134e+00,\n",
      "        1.2145e+01, 9.6824e+00, 8.7805e+00, 5.2879e+00, 7.5193e+00, 7.9378e+00,\n",
      "        7.2640e+00, 4.9018e+00, 5.9908e+00, 6.3472e+00, 7.5579e+00, 1.1493e+01,\n",
      "        7.4386e+00, 1.0459e+01, 8.0694e+00, 8.4796e+00, 4.2591e+00, 6.9986e+00,\n",
      "        4.7788e+00, 7.7015e+00, 3.1528e+00, 5.0023e+00, 8.7947e+00, 8.4771e+00,\n",
      "        3.3599e+00, 7.7846e+00, 1.0000e+01, 3.1755e+00, 6.6088e+00, 6.0602e+00,\n",
      "        4.3883e+00, 5.7759e+00, 8.5024e+00, 6.5438e+00, 7.9996e+00, 8.3258e+00,\n",
      "        5.6873e+00, 6.0661e+00, 8.6890e+00, 9.2171e+00, 7.5872e+00, 6.0871e+00,\n",
      "        4.1080e+00, 7.8402e+00, 8.8311e+00, 6.9163e+00, 9.1462e+00, 8.6870e+00,\n",
      "        5.9079e+00, 6.5995e+00, 6.9502e+00, 7.4703e+00, 8.3344e+00, 5.2559e+00,\n",
      "        7.2719e+00, 3.0786e+00, 3.4469e+00, 6.5164e+00, 8.2747e+00, 6.3070e+00,\n",
      "        5.5913e+00, 7.0327e+00, 5.8071e+00, 6.0420e+00, 7.6413e+00, 5.4750e+00,\n",
      "        1.0121e+01, 1.0234e+01, 4.3456e+00, 1.0729e+01, 4.8077e+00, 7.5405e+00,\n",
      "        3.1822e+00, 6.1709e+00, 7.3522e+00, 9.2152e+00, 4.3245e+00, 7.3626e+00,\n",
      "        3.8005e+00, 4.0664e+00, 3.7872e+00, 6.6614e+00, 8.2837e+00, 5.8579e+00,\n",
      "        5.1730e+00, 6.7811e+00, 3.8725e+00, 5.4263e+00, 8.4823e+00, 1.0219e+01,\n",
      "        4.6000e+00, 7.9922e+00, 5.7237e+00, 8.9825e+00, 4.9716e+00, 8.8096e+00,\n",
      "        3.9615e+00, 4.5021e+00, 8.1191e+00, 6.9806e+00, 5.9960e+00, 5.1199e+00,\n",
      "        3.2731e+00, 9.9959e+00, 9.3592e+00, 5.0220e+00, 5.5741e+00, 5.3109e+00,\n",
      "        6.8338e+00, 9.1559e+00, 5.4694e+00, 7.8777e+00, 7.6644e+00, 3.5291e+00,\n",
      "        4.6393e+00, 7.3286e+00, 8.9947e+00, 4.7538e+00, 9.6250e+00, 3.9282e+00,\n",
      "        3.1783e+00, 1.0228e+01, 5.5733e+00, 6.6088e+00, 4.6208e+00, 7.4257e+00,\n",
      "        5.7517e+00, 7.0408e+00, 5.8245e+00, 5.5135e+00, 9.9807e+00, 3.7504e+00,\n",
      "        4.6213e+00, 8.5605e+00, 6.0093e+00, 8.6930e+00, 8.4902e+00, 4.5743e+00,\n",
      "        6.5848e+00, 4.6965e+00, 6.8051e+00, 8.5218e+00, 5.2914e+00, 4.6858e+00,\n",
      "        3.2730e+00, 7.3968e+00, 5.6783e+00, 4.9192e+00, 8.0140e+00, 7.2397e+00,\n",
      "        6.9612e+00, 4.2762e+00, 5.1731e+00, 5.4166e+00, 6.8653e+00, 9.1719e+00,\n",
      "        4.3951e+00, 6.7821e+00, 6.1644e+00, 5.8530e+00, 8.1215e+00, 8.4226e+00,\n",
      "        5.1110e+00, 5.3597e+00, 8.2940e+00, 6.8941e+00, 7.0462e+00, 7.3623e+00,\n",
      "        1.3104e+01, 7.0154e+00, 3.8298e+00, 7.5906e+00, 6.2257e+00, 3.2968e+00,\n",
      "        8.3191e+00, 4.9226e+00, 3.8058e+00, 9.5821e+00, 4.2320e+00, 6.1204e+00,\n",
      "        4.1587e+00, 5.6560e+00, 4.4607e+00, 2.9166e+00, 8.3253e+00, 1.0084e+01,\n",
      "        7.5445e+00, 8.6579e+00, 7.5809e+00, 7.0912e+00, 9.0382e+00, 8.6694e+00,\n",
      "        7.0198e+00, 9.9448e+00, 7.5635e+00, 4.1335e+00, 7.6553e+00, 7.7148e+00,\n",
      "        4.0236e+00, 5.6054e+00, 5.0198e+00, 9.2309e+00, 6.4839e+00, 5.1503e+00,\n",
      "        8.2335e+00, 4.2936e+00, 7.1300e+00, 9.0241e+00, 6.1912e+00, 4.5784e+00,\n",
      "        9.8304e+00, 4.5151e+00, 6.7815e+00, 9.9836e+00, 7.7569e+00, 6.1879e+00,\n",
      "        7.8038e+00, 5.3811e+00, 8.0341e+00, 3.7527e+00, 3.5097e+00, 1.0664e+01,\n",
      "        5.9181e+00, 8.0867e+00, 7.6975e+00, 9.2299e+00, 6.3334e+00, 7.1928e+00,\n",
      "        8.1445e+00, 7.7797e+00, 4.2157e+00, 8.5365e+00, 3.5396e+00, 5.1873e+00,\n",
      "        7.9775e+00, 4.3339e+00, 5.1652e+00, 8.1413e+00, 8.3540e+00, 6.8088e+00,\n",
      "        6.7496e+00, 5.2867e+00, 6.0156e+00, 6.9955e+00, 8.5251e+00, 1.1360e+01,\n",
      "        5.5588e+00, 8.0784e+00, 5.7467e+00, 9.6325e+00, 3.0512e+00, 3.7503e+00,\n",
      "        7.0788e+00, 8.5262e+00, 9.8982e+00, 9.6863e+00, 8.1303e+00, 7.6488e+00,\n",
      "        6.3582e+00, 6.1194e+00, 4.0742e+00, 8.0770e+00, 3.5652e+00, 3.7590e+00,\n",
      "        2.8930e+00, 8.7095e+00, 8.1377e+00, 5.2958e+00, 6.3672e+00, 7.3450e+00,\n",
      "        2.8658e+00, 7.6311e+00, 5.0861e+00, 7.1259e+00, 8.4500e+00, 6.4476e+00,\n",
      "        6.1605e+00, 6.4577e+00, 6.1688e+00, 4.8468e+00, 9.2931e+00, 8.2559e+00,\n",
      "        3.7026e+00, 8.5010e+00, 9.4960e+00, 5.1272e+00, 7.2589e+00, 5.7951e+00,\n",
      "        4.6381e+00, 3.0936e+00, 3.4529e+00, 5.9862e+00, 8.3970e+00, 9.3004e+00,\n",
      "        6.9713e+00, 8.9232e+00, 5.5300e+00, 7.7534e+00, 3.7689e+00, 5.8400e+00,\n",
      "        8.3358e+00, 5.2824e+00, 9.4945e+00, 6.7043e+00, 7.8609e+00, 9.2997e+00,\n",
      "        8.8080e+00, 6.6764e+00, 1.0151e+01, 3.3668e+00, 6.7202e+00, 7.5316e+00,\n",
      "        8.3490e+00, 8.5153e+00, 8.7581e+00, 7.6977e+00, 8.7996e+00, 9.1567e+00,\n",
      "        3.0874e+00, 6.0595e+00, 3.0035e+00, 4.1330e+00, 8.8163e+00, 5.5010e+00,\n",
      "        7.6868e+00, 5.0337e+00, 8.9457e+00, 7.7209e+00, 1.0050e+01, 7.0169e+00,\n",
      "        6.5493e+00, 6.9043e+00, 6.6136e+00, 5.2510e+00, 4.3140e+00, 6.5390e+00,\n",
      "        2.8391e+00, 1.0343e+01, 5.4496e+00, 4.5676e+00, 5.7098e+00, 3.6576e+00,\n",
      "        3.9493e+00, 6.8291e+00, 1.0382e+01, 5.1224e+00, 6.5623e+00, 9.7174e+00,\n",
      "        9.9735e+00, 7.9977e+00, 4.5017e+00, 3.3549e+00, 4.6744e+00, 8.8701e+00,\n",
      "        5.4342e+00, 5.0070e+00, 7.4269e+00, 7.3679e+00, 5.4214e+00, 5.8185e+00,\n",
      "        9.2593e+00, 7.3697e+00, 3.4387e+00, 5.6394e+00, 6.3946e+00, 3.3000e+00,\n",
      "        5.7886e+00, 9.1730e+00, 5.9174e+00, 9.9814e+00, 7.8277e+00, 9.2831e+00,\n",
      "        8.5700e+00, 6.6982e+00, 4.9196e+00, 3.7775e+00, 6.9368e+00, 5.1924e+00,\n",
      "        6.4024e+00, 1.0385e+01, 1.1934e+01, 5.5366e+00, 4.1737e+00, 4.9594e+00,\n",
      "        1.0234e+01, 8.4627e+00, 5.7348e+00, 4.2715e+00, 9.3044e+00, 8.6998e+00,\n",
      "        3.4034e+00, 5.8665e+00, 3.9747e+00, 5.6717e+00, 1.0553e+01, 4.4846e+00,\n",
      "        7.9363e+00, 8.7658e+00, 3.1663e+00, 6.5752e+00, 4.3472e+00, 5.0901e+00,\n",
      "        9.4828e+00, 9.7793e+00, 7.8727e+00, 6.9667e+00, 4.2022e+00, 9.2740e+00,\n",
      "        3.2983e+00, 4.7403e+00, 6.4180e+00, 5.6795e+00, 1.0241e+01, 5.8445e+00,\n",
      "        3.0343e+00, 8.6907e+00, 5.1124e+00, 3.8764e+00, 3.1164e+00, 4.2462e+00,\n",
      "        4.8587e+00, 6.4313e+00, 8.9931e+00, 4.5609e+00, 8.6762e+00, 3.5467e+00,\n",
      "        8.5593e+00, 7.8400e+00, 7.2848e+00, 9.0972e+00, 8.1743e+00, 8.8269e+00,\n",
      "        7.5785e+00, 5.0360e+00, 5.7740e+00, 3.8530e+00, 4.0241e+00, 6.6091e+00,\n",
      "        3.2837e+00, 3.8713e+00, 6.7925e+00, 5.4090e+00, 6.7943e+00, 6.5869e+00,\n",
      "        7.8137e+00, 7.3886e+00, 4.3213e+00, 5.1981e+00, 9.8652e+00, 4.4415e+00,\n",
      "        8.0227e+00, 6.4493e+00, 9.1543e+00, 4.4335e+00, 6.9932e+00, 9.1066e+00,\n",
      "        5.1405e+00, 9.7773e+00, 4.4003e+00, 6.5465e+00, 5.6653e+00, 5.8466e+00,\n",
      "        6.0156e+00, 6.2083e+00, 5.2941e+00, 9.0589e+00, 8.5242e+00, 1.0106e+01,\n",
      "        3.3591e+00, 3.2218e+00, 3.9146e+00, 8.5018e+00, 6.9517e+00, 3.9240e+00,\n",
      "        7.8862e+00, 6.6528e+00, 5.6085e+00, 4.3080e+00, 1.1968e+01, 3.3243e+00,\n",
      "        7.8229e+00, 1.1615e+01, 4.4619e+00, 8.4154e+00, 4.6758e+00, 9.1840e+00,\n",
      "        7.1253e+00, 6.8298e+00, 8.9708e+00, 8.5195e+00, 7.8991e+00, 4.6627e+00,\n",
      "        8.1540e+00, 4.9519e+00, 4.8971e+00, 3.9593e+00, 7.5038e+00, 5.5633e+00,\n",
      "        9.1113e+00, 6.6887e+00, 4.5805e+00, 7.3886e+00, 8.3848e+00, 9.3924e+00,\n",
      "        4.7998e+00, 3.2955e+00, 3.5297e+00, 5.9936e+00, 6.2204e+00, 3.7166e+00,\n",
      "        2.8445e+00, 6.5643e+00, 4.3247e+00, 4.6628e+00, 4.0250e+00, 5.8499e+00,\n",
      "        5.2762e+00, 8.9126e+00, 7.7265e+00, 1.0057e+01, 6.0062e+00, 5.7264e+00,\n",
      "        7.5039e+00, 4.1111e+00, 7.1690e+00, 4.0899e+00, 5.0789e+00, 8.4104e+00,\n",
      "        4.7404e+00, 3.4460e+00, 6.3608e+00, 8.1593e+00, 1.1021e+01, 5.0076e+00,\n",
      "        8.0111e+00, 5.6931e+00, 3.9179e+00, 3.9791e+00, 9.2750e+00, 8.3386e+00,\n",
      "        6.8970e+00, 4.7545e+00, 5.1980e+00, 6.1840e+00, 7.6008e+00, 5.6990e+00,\n",
      "        7.6608e+00, 6.5492e+00, 4.0209e+00, 6.1758e+00, 8.3565e+00, 8.9856e+00,\n",
      "        3.0999e+00, 5.2339e+00, 8.1144e+00, 4.5853e+00, 2.9962e+00, 9.9684e+00,\n",
      "        8.6617e+00, 6.7697e+00, 3.3135e+00, 7.2230e+00, 3.2375e+00, 4.4254e+00,\n",
      "        4.9353e+00, 4.2130e+00, 7.0001e+00, 9.7725e+00, 7.6168e+00, 9.4275e+00,\n",
      "        1.0353e+01, 5.1499e+00, 4.5861e+00, 9.8779e+00, 6.6050e+00, 7.2949e+00,\n",
      "        4.2527e+00, 7.2104e+00, 7.4069e+00, 4.2695e+00, 7.6364e+00, 8.3255e+00,\n",
      "        4.9968e+00, 8.3864e+00, 4.0006e+00, 6.2861e+00, 6.1989e+00, 3.2834e+00,\n",
      "        6.2516e+00, 5.8286e+00, 6.9322e+00, 5.8837e+00, 5.4610e+00, 5.6622e+00,\n",
      "        8.7425e+00, 1.0003e+01, 6.0365e+00, 2.7891e+00, 6.7916e+00, 5.5352e+00,\n",
      "        9.5424e+00, 8.5779e+00, 6.4315e+00, 6.6357e+00, 8.7932e+00, 4.5167e+00,\n",
      "        7.0889e+00, 6.9475e+00, 8.0037e+00, 3.4234e+00, 3.6229e+00, 1.0066e+01,\n",
      "        8.4116e+00, 4.1813e+00, 9.2081e+00, 8.7983e+00, 5.7752e+00, 8.1433e+00,\n",
      "        5.7133e+00, 9.4769e+00, 3.9125e+00, 7.7905e+00, 6.3868e+00, 4.6467e+00,\n",
      "        6.0536e+00, 5.5983e+00, 5.7288e+00, 5.5982e+00, 4.9214e+00, 7.7548e+00,\n",
      "        3.3550e+00, 6.8575e+00, 5.2985e+00, 4.4027e+00, 3.8131e+00, 4.9659e+00,\n",
      "        7.1691e+00, 9.3298e+00, 4.7811e+00, 4.7900e+00, 8.0367e+00, 4.2682e+00,\n",
      "        7.9399e+00, 3.3789e+00, 1.0620e+01, 5.3640e+00, 6.3082e+00, 8.4152e+00,\n",
      "        8.6579e+00, 5.9692e+00, 6.6329e+00, 8.0041e+00, 8.3637e+00, 3.9946e+00,\n",
      "        1.0888e+01, 4.7510e+00, 3.3234e+00, 4.1619e+00, 4.2313e+00, 7.2531e+00,\n",
      "        5.7621e+00, 7.9141e+00, 5.3181e+00, 8.2106e+00, 7.9045e+00, 7.5215e+00,\n",
      "        8.4947e+00, 3.9915e+00, 6.3549e+00, 9.2590e+00, 5.8759e+00, 7.7673e+00,\n",
      "        4.0007e+00, 8.6709e+00, 8.9033e+00, 7.7392e+00, 8.4067e+00, 6.9263e+00,\n",
      "        7.3029e+00, 4.6038e+00, 5.3484e+00, 9.2653e+00, 1.0037e+01, 3.5802e+00,\n",
      "        4.5318e+00, 6.0077e+00, 5.7482e+00, 7.2233e+00, 3.3636e+00, 4.5350e+00,\n",
      "        4.5317e+00, 3.7069e+00, 3.0985e+00, 8.9666e+00, 5.8642e+00, 7.3966e+00,\n",
      "        7.7568e+00, 7.4780e+00, 6.9810e+00, 9.0533e+00, 6.2262e+00, 6.4652e+00,\n",
      "        8.4744e+00, 9.9354e+00, 6.9853e+00, 8.5528e+00, 4.9262e+00, 7.4856e+00,\n",
      "        3.3446e+00, 2.9970e+00, 7.9901e+00, 4.5803e+00, 3.3601e+00, 8.5920e+00,\n",
      "        4.3383e+00, 4.3936e+00, 9.9116e+00, 5.7646e+00, 8.5688e+00, 4.1097e+00,\n",
      "        8.6539e+00, 9.4817e+00, 5.6577e+00, 6.1296e+00, 7.1885e+00, 4.0171e+00,\n",
      "        6.8322e+00, 9.6312e+00, 4.0066e+00, 5.5410e+00, 8.8266e+00, 8.4339e+00,\n",
      "        7.6316e+00, 3.3264e+00, 6.9340e+00, 8.0658e+00, 2.9796e+00, 3.5211e+00,\n",
      "        1.0884e+01, 7.9772e+00, 5.7605e+00, 4.9912e+00, 5.4710e+00, 8.5723e+00,\n",
      "        6.3894e+00, 5.5866e+00, 7.9757e+00, 7.1598e+00, 7.6992e+00, 6.9973e+00,\n",
      "        7.6617e+00, 7.9859e+00, 7.4586e+00, 4.1486e+00, 9.3260e+00, 7.2541e+00,\n",
      "        1.2579e+01, 3.1438e+00, 7.4611e+00, 6.2264e+00])\n",
      "torch.Size([20000])\n",
      "torch.Size([20000, 910])\n",
      "tensor([1.6000e+01, 5.0964e+04, 3.3528e+00, 4.5759e+00, 5.5767e+00, 6.5311e+00,\n",
      "        5.0238e+00, 3.0094e+00, 3.7428e+00, 7.7165e+00, 6.4655e+00, 5.3495e+00,\n",
      "        6.2151e+00, 6.2000e+00, 9.0788e+00, 6.5243e+00, 3.0344e+00, 3.3850e+00,\n",
      "        8.0237e+00, 8.9541e+00, 7.5563e+00, 8.6466e+00, 7.5915e+00, 8.1167e+00,\n",
      "        7.8650e+00, 3.4388e+00, 4.8105e+00, 8.9826e+00, 6.9050e+00, 9.9288e+00,\n",
      "        6.1963e+00, 6.1915e+00, 5.4930e+00, 7.0298e+00, 4.5937e+00, 4.8683e+00,\n",
      "        3.1380e+00, 5.0210e+00, 7.7881e+00, 5.1182e+00, 5.8215e+00, 7.3052e+00,\n",
      "        6.4116e+00, 6.9526e+00, 8.8632e+00, 7.4313e+00, 4.1118e+00, 3.2987e+00,\n",
      "        8.5543e+00, 8.3143e+00, 5.2479e+00, 2.8329e+00, 5.7689e+00, 2.8699e+00,\n",
      "        3.3769e+00, 8.7412e+00, 5.6821e+00, 8.6810e+00, 5.8652e+00, 8.7554e+00,\n",
      "        3.1191e+00, 8.9120e+00, 6.1849e+00, 6.8125e+00, 6.0736e+00, 9.6926e+00,\n",
      "        3.4089e+00, 5.8668e+00, 4.1144e+00, 3.3230e+00, 6.4284e+00, 4.0300e+00,\n",
      "        6.0513e+00, 6.8145e+00, 5.3266e+00, 3.1085e+00, 3.9956e+00, 6.0700e+00,\n",
      "        7.0360e+00, 5.1000e+00, 3.1270e+00, 1.0959e+01, 7.0647e+00, 8.1073e+00,\n",
      "        3.1131e+00, 8.6101e+00, 4.6501e+00, 4.6659e+00, 7.4733e+00, 9.1803e+00,\n",
      "        3.7843e+00, 1.0633e+01, 1.0009e+01, 6.3680e+00, 7.6991e+00, 6.7845e+00,\n",
      "        5.5010e+00, 4.1873e+00, 6.0718e+00, 9.0738e+00, 4.4208e+00, 8.8679e+00,\n",
      "        2.9089e+00, 7.7741e+00, 4.4614e+00, 3.9556e+00, 6.7289e+00, 5.9347e+00,\n",
      "        3.4713e+00, 3.0574e+00, 6.9023e+00, 1.0478e+01, 4.7063e+00, 7.4717e+00,\n",
      "        4.5618e+00, 3.7508e+00, 7.4273e+00, 2.6673e+00, 3.5083e+00, 3.1269e+00,\n",
      "        8.9985e+00, 5.6635e+00, 3.3246e+00, 3.2406e+00, 8.0407e+00, 3.6047e+00,\n",
      "        3.3730e+00, 9.8515e+00, 8.1335e+00, 3.2354e+00, 5.7923e+00, 3.6372e+00,\n",
      "        1.0272e+01, 8.0086e+00, 7.9125e+00, 6.6387e+00, 7.5445e+00, 8.5125e+00,\n",
      "        5.3864e+00, 3.9395e+00, 3.8036e+00, 3.4963e+00, 4.0577e+00, 9.1576e+00,\n",
      "        9.0512e+00, 4.8829e+00, 8.3323e+00, 1.0830e+01, 6.8648e+00, 4.9982e+00,\n",
      "        1.2827e+01, 3.8391e+00, 8.3974e+00, 5.2422e+00, 7.7686e+00, 6.5328e+00,\n",
      "        3.7333e+00, 6.7516e+00, 4.6200e+00, 6.4188e+00, 3.6598e+00, 1.2140e+01,\n",
      "        8.7308e+00, 1.0506e+01, 6.7635e+00, 9.3683e+00, 4.4032e+00, 8.0344e+00,\n",
      "        3.5691e+00, 8.4642e+00, 3.7362e+00, 5.0372e+00, 9.0523e+00, 9.7979e+00,\n",
      "        3.9617e+00, 7.9761e+00, 1.0336e+01, 3.9350e+00, 1.0497e+01, 7.7299e+00,\n",
      "        4.5418e+00, 4.7727e+00, 3.7862e+00, 5.6849e+00, 8.9757e+00, 6.3227e+00,\n",
      "        6.0544e+00, 5.0624e+00, 9.3569e+00, 8.8488e+00, 3.5819e+00, 6.7183e+00,\n",
      "        3.1087e+00, 8.0798e+00, 4.6827e+00, 6.3036e+00, 7.8911e+00, 8.6943e+00,\n",
      "        5.6009e+00, 7.0382e+00, 5.2915e+00, 5.2025e+00, 8.6922e+00, 5.4305e+00,\n",
      "        8.0498e+00, 3.6808e+00, 3.7519e+00, 1.0194e+01, 9.2235e+00, 6.0132e+00,\n",
      "        6.5676e+00, 8.3911e+00, 5.1833e+00, 3.8679e+00, 7.7752e+00, 5.5394e+00,\n",
      "        9.5489e+00, 8.5529e+00, 4.5163e+00, 8.5944e+00, 4.7835e+00, 8.6575e+00,\n",
      "        9.3942e+00, 5.6895e+00, 6.5135e+00, 9.4706e+00, 4.6458e+00, 6.9426e+00,\n",
      "        3.7437e+00, 3.9939e+00, 4.0432e+00, 6.9830e+00, 6.3321e+00, 7.1658e+00,\n",
      "        6.6166e+00, 4.7190e+00, 3.3701e+00, 6.7666e+00, 3.1320e+00, 8.8562e+00,\n",
      "        5.3791e+00, 8.0590e+00, 6.5740e+00, 9.0856e+00, 5.2626e+00, 1.0195e+01,\n",
      "        5.4515e+00, 5.1587e+00, 8.0972e+00, 7.5084e+00, 6.1303e+00, 5.2670e+00,\n",
      "        3.4787e+00, 1.1484e+01, 7.1376e+00, 5.8718e+00, 7.3861e+00, 4.9590e+00,\n",
      "        8.0897e+00, 9.1918e+00, 7.7839e+00, 3.4791e+00, 3.7906e+00, 3.3480e+00,\n",
      "        7.5775e+00, 7.1918e+00, 3.2097e+00, 4.1511e+00, 8.5377e+00, 7.3721e+00,\n",
      "        3.0120e+00, 4.6129e+00, 6.2192e+00, 3.4422e+00, 6.0563e+00, 5.9516e+00,\n",
      "        4.0190e+00, 3.5407e+00, 5.6068e+00, 4.2258e+00, 1.1286e+01, 3.2316e+00,\n",
      "        5.5592e+00, 7.3136e+00, 7.2384e+00, 1.0512e+01, 7.8267e+00, 5.6981e+00,\n",
      "        8.5891e+00, 4.9513e+00, 8.0835e+00, 9.5240e+00, 7.4842e+00, 5.6330e+00,\n",
      "        3.1683e+00, 3.3560e+00, 5.4091e+00, 3.5521e+00, 9.6214e+00, 5.0401e+00,\n",
      "        6.9297e+00, 3.0719e+00, 3.0884e+00, 3.8987e+00, 4.3794e+00, 6.9996e+00,\n",
      "        4.6855e+00, 3.1863e+00, 6.2594e+00, 5.6830e+00, 7.7929e+00, 7.0382e+00,\n",
      "        4.9504e+00, 5.8923e+00, 7.7708e+00, 6.6091e+00, 7.8523e+00, 1.0380e+01,\n",
      "        1.2756e+01, 5.4537e+00, 4.7133e+00, 6.3735e+00, 8.2298e+00, 3.2178e+00,\n",
      "        1.0363e+01, 4.9310e+00, 3.9457e+00, 1.0580e+01, 6.4487e+00, 6.5425e+00,\n",
      "        5.4107e+00, 5.9554e+00, 5.4179e+00, 3.1504e+00, 8.1255e+00, 8.8501e+00,\n",
      "        8.6074e+00, 7.7413e+00, 3.7534e+00, 7.0345e+00, 6.4847e+00, 8.9396e+00,\n",
      "        5.8023e+00, 1.0406e+01, 8.0463e+00, 4.5356e+00, 7.3838e+00, 7.8119e+00,\n",
      "        5.1363e+00, 5.3530e+00, 5.3543e+00, 9.7198e+00, 5.0174e+00, 5.7285e+00,\n",
      "        7.0409e+00, 4.1899e+00, 8.6277e+00, 3.3583e+00, 5.7691e+00, 3.8901e+00,\n",
      "        1.0084e+01, 5.8236e+00, 5.8059e+00, 9.5688e+00, 1.0034e+01, 5.0139e+00,\n",
      "        7.2634e+00, 5.4524e+00, 6.9710e+00, 4.1522e+00, 2.7732e+00, 8.6479e+00,\n",
      "        8.1868e+00, 6.6864e+00, 4.0525e+00, 8.6515e+00, 5.7811e+00, 4.7966e+00,\n",
      "        7.4557e+00, 7.3993e+00, 3.8278e+00, 8.2424e+00, 3.9622e+00, 4.7546e+00,\n",
      "        9.1122e+00, 5.8771e+00, 4.5526e+00, 7.7826e+00, 9.2731e+00, 7.8702e+00,\n",
      "        5.3535e+00, 5.4416e+00, 7.5235e+00, 8.3700e+00, 7.2439e+00, 6.0966e+00,\n",
      "        8.6692e+00, 6.8212e+00, 7.0035e+00, 8.4297e+00, 3.0159e+00, 5.8750e+00,\n",
      "        8.0102e+00, 9.3814e+00, 8.5180e+00, 9.8078e+00, 6.1492e+00, 7.0556e+00,\n",
      "        3.8735e+00, 4.8720e+00, 4.4385e+00, 3.8542e+00, 3.6660e+00, 3.8003e+00,\n",
      "        3.2300e+00, 8.9450e+00, 8.2184e+00, 5.6740e+00, 8.0082e+00, 8.2053e+00,\n",
      "        1.1351e+01, 7.5063e+00, 5.1269e+00, 7.3183e+00, 5.0783e+00, 7.8631e+00,\n",
      "        8.6588e+00, 6.8184e+00, 6.2737e+00, 4.9090e+00, 9.1254e+00, 8.5782e+00,\n",
      "        3.7535e+00, 1.1084e+01, 9.1094e+00, 2.9868e+00, 8.3425e+00, 3.7011e+00,\n",
      "        3.3289e+00, 3.2686e+00, 3.3283e+00, 6.4590e+00, 9.0846e+00, 8.8574e+00,\n",
      "        7.0781e+00, 8.1532e+00, 6.1914e+00, 7.8346e+00, 3.2493e+00, 5.4511e+00,\n",
      "        9.3460e+00, 2.9453e+00, 7.5686e+00, 3.1738e+00, 8.8898e+00, 9.8755e+00,\n",
      "        7.6629e+00, 5.4953e+00, 9.7714e+00, 4.2466e+00, 6.6470e+00, 4.2362e+00,\n",
      "        7.6873e+00, 8.5791e+00, 7.9667e+00, 8.3475e+00, 6.5838e+00, 7.8900e+00,\n",
      "        3.4000e+00, 6.8933e+00, 4.5443e+00, 3.9548e+00, 1.0438e+01, 4.5279e+00,\n",
      "        4.1988e+00, 4.4367e+00, 9.4230e+00, 3.3611e+00, 1.1362e+01, 6.8162e+00,\n",
      "        6.0071e+00, 3.4555e+00, 5.6734e+00, 5.3026e+00, 5.3561e+00, 7.4023e+00,\n",
      "        3.0105e+00, 1.1883e+01, 6.5486e+00, 4.5684e+00, 6.6447e+00, 3.7209e+00,\n",
      "        7.5517e+00, 2.9322e+00, 6.9778e+00, 3.5857e+00, 7.1919e+00, 9.0372e+00,\n",
      "        8.9601e+00, 4.6741e+00, 4.9488e+00, 3.4248e+00, 5.2198e+00, 7.5679e+00,\n",
      "        6.4987e+00, 6.7370e+00, 3.5667e+00, 7.6972e+00, 6.5556e+00, 7.2954e+00,\n",
      "        1.0580e+01, 9.1449e+00, 3.7958e+00, 3.3113e+00, 7.9464e+00, 3.4591e+00,\n",
      "        6.6797e+00, 9.3859e+00, 3.5483e+00, 9.0908e+00, 7.9626e+00, 1.0216e+01,\n",
      "        7.7245e+00, 6.5630e+00, 3.7891e+00, 3.2757e+00, 7.2467e+00, 5.6145e+00,\n",
      "        7.5365e+00, 1.1402e+01, 1.3002e+01, 7.3544e+00, 3.4243e+00, 4.9644e+00,\n",
      "        6.3161e+00, 6.9674e+00, 5.4116e+00, 4.1133e+00, 7.0385e+00, 9.7940e+00,\n",
      "        2.9155e+00, 5.6429e+00, 4.9909e+00, 6.6497e+00, 8.7693e+00, 3.1762e+00,\n",
      "        8.0741e+00, 9.7528e+00, 2.9227e+00, 3.7020e+00, 6.0665e+00, 4.2494e+00,\n",
      "        9.7000e+00, 9.3991e+00, 7.6235e+00, 3.3766e+00, 4.0203e+00, 8.6022e+00,\n",
      "        3.8176e+00, 5.1802e+00, 5.5316e+00, 5.4432e+00, 9.9317e+00, 4.0665e+00,\n",
      "        4.1550e+00, 9.0631e+00, 5.5023e+00, 3.6911e+00, 3.1138e+00, 4.2985e+00,\n",
      "        4.7644e+00, 7.3784e+00, 3.4842e+00, 4.6160e+00, 8.5060e+00, 3.3142e+00,\n",
      "        8.9135e+00, 4.0429e+00, 8.6345e+00, 5.3481e+00, 9.5863e+00, 8.7544e+00,\n",
      "        7.9908e+00, 5.7175e+00, 8.3853e+00, 4.2286e+00, 3.1138e+00, 8.2064e+00,\n",
      "        3.5675e+00, 4.3675e+00, 7.7319e+00, 7.1771e+00, 9.6834e+00, 8.6295e+00,\n",
      "        7.8344e+00, 4.4064e+00, 4.9193e+00, 5.6670e+00, 9.6474e+00, 4.4878e+00,\n",
      "        9.1300e+00, 3.6377e+00, 9.3860e+00, 3.6618e+00, 9.8020e+00, 6.6160e+00,\n",
      "        4.5722e+00, 9.6742e+00, 5.4091e+00, 6.5519e+00, 5.6769e+00, 3.4382e+00,\n",
      "        2.9444e+00, 7.2198e+00, 5.2697e+00, 8.7787e+00, 7.8766e+00, 9.3956e+00,\n",
      "        3.5478e+00, 3.4882e+00, 3.9085e+00, 7.3057e+00, 4.1144e+00, 3.9755e+00,\n",
      "        7.5025e+00, 8.6008e+00, 6.6098e+00, 3.5554e+00, 1.0626e+01, 3.1488e+00,\n",
      "        7.9043e+00, 3.2798e+00, 4.0173e+00, 3.5013e+00, 6.6540e+00, 8.2762e+00,\n",
      "        8.2976e+00, 3.4837e+00, 6.2126e+00, 8.0384e+00, 9.8715e+00, 4.9694e+00,\n",
      "        8.5461e+00, 4.2902e+00, 5.2908e+00, 4.1534e+00, 7.2352e+00, 3.7482e+00,\n",
      "        8.5269e+00, 9.2938e+00, 5.5356e+00, 8.1110e+00, 4.9919e+00, 8.2987e+00,\n",
      "        6.3344e+00, 4.8938e+00, 3.7678e+00, 6.1107e+00, 6.9802e+00, 3.3567e+00,\n",
      "        3.0728e+00, 5.5102e+00, 4.0306e+00, 8.6632e+00, 3.9461e+00, 5.6183e+00,\n",
      "        4.0018e+00, 5.8868e+00, 5.8519e+00, 1.1038e+01, 5.8544e+00, 4.9120e+00,\n",
      "        6.9153e+00, 3.9614e+00, 8.3944e+00, 4.7480e+00, 5.9777e+00, 5.8931e+00,\n",
      "        3.5970e+00, 3.1689e+00, 8.0881e+00, 8.7403e+00, 7.0849e+00, 5.7249e+00,\n",
      "        7.3308e+00, 6.9074e+00, 3.9083e+00, 5.4035e+00, 1.0193e+01, 3.3567e+00,\n",
      "        6.4593e+00, 4.9002e+00, 3.8470e+00, 4.9572e+00, 8.8235e+00, 5.6614e+00,\n",
      "        8.0598e+00, 5.1703e+00, 3.3257e+00, 6.4650e+00, 9.4115e+00, 8.3085e+00,\n",
      "        9.1309e+00, 5.3749e+00, 6.9066e+00, 7.3838e+00, 3.4842e+00, 1.0632e+01,\n",
      "        7.8035e+00, 7.8717e+00, 3.7685e+00, 7.5363e+00, 3.3008e+00, 3.5080e+00,\n",
      "        6.2876e+00, 9.5707e+00, 6.0422e+00, 7.9723e+00, 7.4505e+00, 8.9938e+00,\n",
      "        1.0643e+01, 4.9866e+00, 3.6987e+00, 3.3626e+00, 7.1359e+00, 8.9976e+00,\n",
      "        4.4603e+00, 4.9922e+00, 6.1043e+00, 4.9222e+00, 7.9808e+00, 8.5458e+00,\n",
      "        4.1839e+00, 9.6521e+00, 4.0170e+00, 6.5732e+00, 5.4708e+00, 3.2398e+00,\n",
      "        6.0747e+00, 4.9396e+00, 8.5601e+00, 7.1609e+00, 5.9334e+00, 5.5488e+00,\n",
      "        8.8761e+00, 7.5401e+00, 6.9470e+00, 3.2323e+00, 7.7317e+00, 5.0165e+00,\n",
      "        6.2207e+00, 6.3177e+00, 7.0321e+00, 7.0719e+00, 3.4182e+00, 9.1842e+00,\n",
      "        4.0839e+00, 3.4423e+00, 6.5856e+00, 3.4220e+00, 3.8639e+00, 7.2523e+00,\n",
      "        9.0686e+00, 3.2625e+00, 8.0228e+00, 8.0859e+00, 4.4613e+00, 7.2692e+00,\n",
      "        6.3120e+00, 8.6051e+00, 3.4371e+00, 8.1944e+00, 4.9294e+00, 3.9670e+00,\n",
      "        7.2341e+00, 6.6672e+00, 5.5629e+00, 5.8174e+00, 5.9722e+00, 7.5646e+00,\n",
      "        3.6566e+00, 6.4264e+00, 7.0286e+00, 4.2823e+00, 3.3507e+00, 6.1630e+00,\n",
      "        6.6233e+00, 7.9552e+00, 4.6428e+00, 3.8858e+00, 9.7319e+00, 5.5508e+00,\n",
      "        7.3710e+00, 3.6222e+00, 1.1745e+01, 5.1334e+00, 8.7392e+00, 6.3503e+00,\n",
      "        7.0786e+00, 3.3529e+00, 6.2688e+00, 8.8914e+00, 3.5576e+00, 4.2799e+00,\n",
      "        1.0305e+01, 4.6928e+00, 3.8525e+00, 3.9030e+00, 4.9838e+00, 3.4016e+00,\n",
      "        3.8199e+00, 7.4299e+00, 4.3648e+00, 1.0329e+01, 8.3794e+00, 9.5700e+00,\n",
      "        9.2923e+00, 3.1691e+00, 7.8062e+00, 9.8768e+00, 4.5749e+00, 7.9668e+00,\n",
      "        3.9895e+00, 7.8825e+00, 4.4768e+00, 3.9744e+00, 9.8713e+00, 3.4107e+00,\n",
      "        7.7165e+00, 5.4271e+00, 5.7268e+00, 9.1966e+00, 9.3337e+00, 4.4537e+00,\n",
      "        5.9488e+00, 6.4124e+00, 5.7915e+00, 6.4756e+00, 3.7836e+00, 3.8969e+00,\n",
      "        4.9854e+00, 3.2755e+00, 6.7285e+00, 9.0393e+00, 6.7707e+00, 7.0138e+00,\n",
      "        7.6408e+00, 3.8995e+00, 7.6685e+00, 8.1655e+00, 6.6703e+00, 4.8812e+00,\n",
      "        3.6304e+00, 5.9016e+00, 7.0629e+00, 8.4321e+00, 6.7382e+00, 7.8248e+00,\n",
      "        3.9680e+00, 3.0185e+00, 6.8003e+00, 3.6088e+00, 5.3620e+00, 7.5160e+00,\n",
      "        3.5589e+00, 3.8852e+00, 8.9565e+00, 4.4865e+00, 4.4237e+00, 4.3102e+00,\n",
      "        8.3266e+00, 9.5601e+00, 7.2798e+00, 8.3376e+00, 8.1591e+00, 3.5793e+00,\n",
      "        5.6442e+00, 6.6767e+00, 3.5733e+00, 5.9275e+00, 9.5475e+00, 8.4704e+00,\n",
      "        9.8181e+00, 3.9459e+00, 8.3105e+00, 9.0326e+00, 1.1969e+01, 3.4203e+00,\n",
      "        3.1448e+00, 8.8994e+00, 4.4566e+00, 3.9582e+00, 7.1626e+00, 9.6808e+00,\n",
      "        6.3188e+00, 5.7239e+00, 7.2855e+00, 7.4175e+00, 7.7026e+00, 3.4837e+00,\n",
      "        7.7782e+00, 8.9690e+00, 7.2644e+00, 3.9075e+00, 4.6317e+00, 6.9228e+00,\n",
      "        1.2188e+01, 3.3633e+00, 8.9436e+00, 7.6205e+00])\n",
      "torch.Size([20000])\n",
      "torch.Size([20000, 910])\n",
      "tensor([ 10.0000, 351.0000,   7.0129,   3.9810,   7.9036,   5.4686,   4.2308,\n",
      "          2.9414,   6.4587,   6.8352,   6.2356,   7.0237,   7.3349,   5.3529,\n",
      "          8.5231,   6.3567,   3.9769,   3.3935,   7.2448,   6.4915,   7.8003,\n",
      "          6.9384,   6.5620,   5.8181,   7.9801,   4.6208,   3.7930,   8.8801,\n",
      "          7.0513,   2.6956,   9.2094,   7.1193,   4.7488,   7.6068,   4.5451,\n",
      "          7.6742,   3.3366,   5.4680,   6.6340,   5.0702,   9.4375,   6.5079,\n",
      "          5.3839,   7.0963,   9.1712,   5.5484,   3.4887,   3.2230,   8.0802,\n",
      "          5.6019,   6.9643,   4.7313,   6.6446,   3.0560,   4.5647,   8.9449,\n",
      "          5.3847,   9.0442,   4.3796,   8.6705,   3.2402,   9.3238,   5.8029,\n",
      "          8.4275,   4.9719,   7.8999,   3.1254,   4.9580,   4.9177,   4.9658,\n",
      "          6.7970,   4.2089,   6.2459,   7.2770,   4.9054,   3.4458,   4.2460,\n",
      "          7.7669,   6.8401,   3.6052,   2.9407,  10.7375,   6.0809,   7.6770,\n",
      "          3.4269,   8.0502,   4.8651,   4.4618,   8.6958,   8.7086,   3.9640,\n",
      "          9.7430,   8.9974,   5.7763,   6.7964,   4.6331,   5.9521,   6.1766,\n",
      "          6.8887,   8.1939,   4.6865,   7.8803,   3.7349,   6.4419,   4.6870,\n",
      "          4.4076,   6.3777,   6.4858,   3.5847,   3.0901,   7.4817,  10.9561,\n",
      "          7.5180,   7.8373,   3.5378,   3.7201,   6.3005,   2.6852,   3.5973,\n",
      "          3.1096,   8.5372,   5.4125,   5.1161,   3.0533,   7.7589,   3.3351,\n",
      "          3.0309,   9.1242,   8.0844,   3.4102,   5.8168,   3.4499,   9.3224,\n",
      "          9.8286,   7.7671,   5.3187,   7.9564,   7.6091,   5.8000,   3.9279,\n",
      "          6.8183,   3.6534,   3.0266,   2.8343,   7.5395,   5.0919,   8.6710,\n",
      "         10.6861,   6.3555,   6.1654,  11.7416,   7.4876,   8.9051,   5.8729,\n",
      "          7.4344,   6.4030,   6.4523,   6.0807,   7.0168,   7.4345,   3.2752,\n",
      "         11.6838,   7.0596,  10.7347,   8.4493,  10.1465,   5.0417,   7.1524,\n",
      "          3.7271,   8.5648,   3.0968,   5.6394,   8.9562,   8.9896,   3.7884,\n",
      "          8.6672,   9.9490,   3.2280,  10.7498,   6.7885,   4.9254,   8.1572,\n",
      "          7.0786,   5.5362,   8.9719,   3.4343,   5.7809,   3.8265,   7.9964,\n",
      "          7.8816,   4.0464,   6.8422,   3.6139,   8.5046,   6.0057,   6.6420,\n",
      "          9.9660,   7.9093,   6.5723,   7.6734,   7.7742,   6.3798,   9.3272,\n",
      "          5.4792,   7.9915,   3.6197,   3.2080,   7.6548,   8.3186,   5.7580,\n",
      "          5.7042,   6.5743,   6.2043,   3.5912,   7.3639,   4.8962,   9.1344,\n",
      "          9.7047,   4.2694,   9.4788,   5.5296,   8.1593,   5.6466,   5.0409,\n",
      "          4.2682,   8.9275,   4.5141,   6.9089,   3.3305,   4.2320,   3.3750,\n",
      "          6.6951,   6.3051,   8.5325,   4.8036,   3.5055,   3.0151,   6.4417,\n",
      "          5.8046,   6.6361,   4.3699,   6.4133,   6.8682,  10.0764,   4.9556,\n",
      "          9.0137,   4.5552,   3.2440,   8.8738,   9.5263,   6.4854,   4.9253,\n",
      "          5.8346,  11.2823,   6.4889,   4.6868,   7.6249,   7.5302,   3.3890,\n",
      "          9.4487,   4.5264,   6.6798,   3.7450,   4.0789,   5.3450,   7.3630,\n",
      "          3.3619,   4.8749,   8.7753,   6.2480,   3.1653,   4.6049,   7.2764,\n",
      "          5.9335,   5.0193,   6.6352,   5.2741,   7.2356,   5.9486,   7.9943,\n",
      "          7.8563,   2.8993,   4.5194,   4.3868,   5.9853,   9.9656,   7.9408,\n",
      "          5.1521,   8.0344,   5.3679,   7.3470,   7.7808,   6.1937,   4.3761,\n",
      "          3.3399,   2.9506,   4.8270,   3.5258,   7.7674,   6.4510,   6.9980,\n",
      "          2.9341,   3.3892,   7.2081,   6.3905,   6.7515,   4.2878,   2.8437,\n",
      "          3.7754,   4.6679,   7.7589,   7.2923,   4.1675,   5.8238,   7.5172,\n",
      "          8.9533,   8.9065,   9.6543,  12.8490,   5.2875,   4.0974,   5.4939,\n",
      "          8.2006,   3.0390,   9.2259,   5.6818,   4.0177,  10.5826,   3.2574,\n",
      "          6.1898,   3.4660,   5.5864,   6.8348,   2.9945,   8.3798,   9.9843,\n",
      "          8.5664,   7.4418,   6.3578,   6.7857,   8.2798,   9.0351,   5.1856,\n",
      "         10.8209,   6.7392,   6.2710,   6.1941,   7.8708,   3.5857,   3.9760,\n",
      "          5.5032,   8.9388,   6.1293,   5.7895,   6.5475,   4.0826,   9.2553,\n",
      "          2.9408,   6.3575,   6.8226,   8.7501,   4.3250,   5.8244,   9.9391,\n",
      "          8.6068,   4.9488,   7.0789,   5.7055,   8.0590,   4.3041,   2.4552,\n",
      "          9.4992,   8.1256,   5.9096,   5.1540,   8.5488,   5.8947,   3.6931,\n",
      "          6.7835,   7.7939,   4.6660,   9.1009,   4.8694,   4.8629,   9.8369,\n",
      "          4.8936,   4.4941,   8.9034,   9.8184,   7.8131,   5.8329,   4.1433,\n",
      "          7.1590,   6.1332,   7.4776,   2.9089,   7.7097,   7.7672,   5.1944,\n",
      "          9.8643,   2.8993,   5.3251,   6.5650,   8.9031,   9.5307,  10.0418,\n",
      "          4.5202,   8.3783,   4.7596,   7.7180,   5.8919,   8.0566,   3.5753,\n",
      "          3.3180,   3.0355,   8.2037,   6.7333,   5.8866,   6.7845,   8.2192,\n",
      "          9.2172,   7.1344,   4.7950,   5.8601,   3.7411,   6.4011,   6.5753,\n",
      "          8.4404,   4.1551,   4.6235,   8.7991,   7.3702,   3.4581,  11.2670,\n",
      "          9.4727,   3.4026,   7.9605,   4.5416,   3.4849,   3.0785,   4.4611,\n",
      "          5.1556,   7.8800,   8.2257,   7.4473,   8.3592,   7.2792,   7.7089,\n",
      "          5.8985,   5.8094,   9.6586,   2.8635,   9.2724,   5.2664,   6.4840,\n",
      "          8.7150,   7.7224,   9.3238,   9.8018,   3.3382,   7.7207,   6.5750,\n",
      "          7.6279,   9.9753,   8.5664,   8.0893,   5.8758,   8.6437,   3.7328,\n",
      "          6.9343,   3.3223,   4.7221,  10.6013,   4.9590,   6.2567,   4.2717,\n",
      "          8.1987,   4.0124,  11.4312,   7.1356,   6.0519,   3.2700,   6.2076,\n",
      "          5.1110,   4.2416,   7.1249,   7.3060,  11.3824,   6.6418,   4.5054,\n",
      "          5.2919,   3.8369,   5.3616,   2.9302,   9.7805,   3.3351,   5.4137,\n",
      "          9.3483,   7.5183,   4.9994,   3.5566,   3.4045,   8.6693,   6.3635,\n",
      "          6.6061,   3.9134,   7.0242,   8.9450,   7.1857,   8.1367,   8.5030,\n",
      "          8.4632,   4.4178,   7.7060,   7.2271,   3.7650,   5.8917,   8.1852,\n",
      "          4.8464,   8.0390,   6.7521,   8.9616,   7.6287,   6.6367,   5.5397,\n",
      "          3.3207,   7.6317,   4.1893,   6.7764,   4.7578,  12.3562,   4.8245,\n",
      "          3.3332,   4.5049,   5.2459,   7.0437,   5.6083,   4.0787,   6.4524,\n",
      "         10.3043,   4.3860,   5.3563,   3.3124,   6.0311,   9.1548,   3.4845,\n",
      "          8.4481,   8.8393,   2.8864,   5.7748,   3.8456,   3.7010,   9.7361,\n",
      "         10.2571,   7.7764,   6.0090,   3.8371,   8.3326,   3.1255,   4.6521,\n",
      "          7.5197,   4.4234,   9.7253,   3.3587,   3.2908,   8.6079,   5.1032,\n",
      "          3.4138,   4.5713,   3.9058,   3.9256,   6.4846,   9.5492,   5.2260,\n",
      "          7.8995,   3.3998,   9.9084,   7.9234,   9.1097,   9.1261,   9.7426,\n",
      "          7.1039,   7.6123,   5.8183,   5.7453,   3.4732,   3.2097,   8.2291,\n",
      "          4.8090,   3.5666,   7.2875,   6.9063,   8.2774,   7.1989,   6.3697,\n",
      "          5.2987,   4.1927,   5.6366,   9.2354,   4.7766,   9.4565,   3.7686,\n",
      "          9.1644,   4.1638,   7.0980,   5.0077,   3.8764,   7.2100,   3.7743,\n",
      "          3.4512,   4.8841,   3.3399,   2.9960,   7.2977,   6.3248,   8.5894,\n",
      "          6.7452,   9.6399,   9.0779,   3.7258,   3.8199,   6.9307,   6.5870,\n",
      "          3.7315,   7.2278,   6.8092,   6.0066,   4.3011,  10.7520,   3.1995,\n",
      "          5.7315,   4.6954,   4.1153,   8.8930,   5.8058,   7.3707,   5.7909,\n",
      "          6.5489,   6.3784,   7.1918,   7.8522,   5.4088,   8.5217,   4.3930,\n",
      "          4.3242,   4.6144,   5.2846,   3.5900,  10.2440,   8.9051,   6.2537,\n",
      "          6.7168,   5.9828,   5.3370,   4.8264,   3.1706,   6.0117,   6.0646,\n",
      "          5.6911,   4.5567,   5.8887,   5.0055,   4.1098,   5.2828,   3.9840,\n",
      "          5.3771,   6.0966,   7.3781,   6.0061,  10.1219,   4.9990,   6.6928,\n",
      "          6.7514,   4.2923,   7.2252,   6.5022,   4.5454,   5.5457,   3.9261,\n",
      "          3.1196,   7.3182,   7.7566,   7.1055,   5.3352,   6.9752,   6.0076,\n",
      "          3.4078,   3.3678,  10.8303,   7.0233,   5.8911,   6.4504,   4.0221,\n",
      "          7.4800,   8.5292,   3.3523,   7.7858,   6.7912,   3.6654,   6.6636,\n",
      "          9.2722,   9.2979,   8.5013,   4.5404,   8.4490,   5.4328,   3.1177,\n",
      "          9.4504,   6.4976,   6.7462,   3.4199,   7.2566,   3.1307,   4.2847,\n",
      "          8.5701,   7.8308,   7.8257,  10.3084,   9.3626,  10.2057,  10.0855,\n",
      "          5.3878,   4.1465,   7.0526,   7.6809,   6.4925,   4.3698,   6.9772,\n",
      "          6.0640,   5.0216,   7.5528,   9.2425,   4.5063,   9.8615,   3.8561,\n",
      "          5.2415,   4.1806,   6.5563,   6.2081,   4.2358,   7.6259,   6.9964,\n",
      "          5.2825,   5.4226,   9.5770,  10.7445,   6.7670,   4.0342,   7.0773,\n",
      "          5.0401,   3.4583,   6.3510,   2.9256,   6.3382,   4.5525,   7.0157,\n",
      "          3.3681,   6.6139,   7.5660,   3.8800,   3.6032,   5.8690,   9.0895,\n",
      "          3.9827,   8.9812,   7.8947,   3.7503,   5.6767,   6.9589,   5.4986,\n",
      "          3.4078,   6.9139,   6.0698,   4.1037,   6.0995,   5.7502,   6.1192,\n",
      "          6.9533,   7.1393,   7.1178,   4.0045,   5.7630,   7.3379,   4.2744,\n",
      "          3.9932,   6.9944,   6.3145,   2.7597,   3.5466,   4.1541,   7.6506,\n",
      "          4.4833,   8.1575,   4.5561,  12.0599,   4.6988,   9.0324,   5.4900,\n",
      "          6.9665,   3.3799,   4.2384,   8.1207,   7.2439,   4.1703,   9.5634,\n",
      "          3.7248,   4.1425,   6.2390,   3.4872,   7.8979,   6.2936,   6.1218,\n",
      "          7.1684,   9.4070,   5.9297,   9.5843,   6.3523,   3.7204,   6.4976,\n",
      "          9.5163,   4.2496,   7.3229,   4.8290,   7.1377,   6.7494,   6.2669,\n",
      "          8.1154,   3.9053,   7.6927,   6.0025,   4.4571,   7.9366,   8.9700,\n",
      "          3.6398,   5.6846,   6.5781,   5.9751,   6.9095,   3.3612,   3.7591,\n",
      "          6.3626,   5.5083,   2.9164,   7.4774,   5.7630,   8.0735,   6.3215,\n",
      "          3.5385,   7.2655,   9.0091,   6.5546,   8.8144,   6.9935,   9.1082,\n",
      "          6.3188,   7.1041,   7.4251,   7.7661,   4.2434,   3.0391,   6.9246,\n",
      "          4.2227,   3.6507,   6.8668,   4.4225,   3.7022,   7.8882,   4.3163,\n",
      "          3.5562,   4.1711,   9.0792,   8.9962,   5.6515,   8.6136,   7.3118,\n",
      "          3.6832,   6.3633,   7.5443,   3.5492,   8.1884,   8.2249,  10.5149,\n",
      "          6.4049,   5.3441,   7.0664,   3.5812,   2.8753,   3.3252,  11.0232,\n",
      "          8.0240,   6.8099,   5.4569,   7.7285,   9.2195,   6.6190,   5.6404,\n",
      "         10.2122,   8.1831,   7.9333,   4.4130,   7.5336,   8.8626,   5.8228,\n",
      "          3.6605,   8.3688,   5.8049,  12.8666,   4.0837,   3.0183,   5.1311])\n",
      "torch.Size([20000])\n",
      "torch.Size([20000, 910])\n",
      "tensor([2.0000e+00, 9.0985e+04, 5.2468e+00, 3.8203e+00, 7.0574e+00, 5.5568e+00,\n",
      "        5.3686e+00, 3.0986e+00, 5.6761e+00, 8.5616e+00, 4.6759e+00, 7.0862e+00,\n",
      "        6.6504e+00, 6.0248e+00, 7.9951e+00, 7.1960e+00, 4.5893e+00, 3.4095e+00,\n",
      "        6.6906e+00, 6.8650e+00, 7.6806e+00, 7.8019e+00, 6.6083e+00, 5.0803e+00,\n",
      "        8.5028e+00, 5.6821e+00, 5.1519e+00, 7.6592e+00, 5.3896e+00, 2.7310e+00,\n",
      "        5.0718e+00, 5.7640e+00, 4.4446e+00, 5.5875e+00, 4.9279e+00, 6.4690e+00,\n",
      "        3.0920e+00, 4.9365e+00, 8.1828e+00, 5.1591e+00, 7.6883e+00, 6.7556e+00,\n",
      "        8.7675e+00, 7.8500e+00, 1.1239e+01, 6.7928e+00, 3.4006e+00, 3.7404e+00,\n",
      "        6.9566e+00, 7.4924e+00, 7.1011e+00, 4.6419e+00, 6.6042e+00, 2.9925e+00,\n",
      "        7.5977e+00, 6.5439e+00, 5.4403e+00, 9.4659e+00, 5.8661e+00, 9.4122e+00,\n",
      "        2.7935e+00, 9.8703e+00, 5.0782e+00, 6.0342e+00, 5.2383e+00, 8.5444e+00,\n",
      "        3.3880e+00, 6.6089e+00, 3.9918e+00, 3.2023e+00, 6.6917e+00, 5.2216e+00,\n",
      "        5.2966e+00, 7.2142e+00, 4.3156e+00, 4.5461e+00, 4.5658e+00, 7.4451e+00,\n",
      "        5.8429e+00, 3.5138e+00, 4.5468e+00, 1.0605e+01, 6.5021e+00, 7.1968e+00,\n",
      "        3.1236e+00, 8.7654e+00, 5.0999e+00, 5.1854e+00, 7.3689e+00, 8.7119e+00,\n",
      "        4.5910e+00, 9.0611e+00, 1.0109e+01, 6.5545e+00, 6.9693e+00, 6.0006e+00,\n",
      "        5.4818e+00, 5.3807e+00, 5.4381e+00, 9.1009e+00, 5.7564e+00, 9.5757e+00,\n",
      "        2.5631e+00, 6.6072e+00, 4.4544e+00, 4.6369e+00, 6.6584e+00, 7.4041e+00,\n",
      "        3.3407e+00, 3.0193e+00, 4.6471e+00, 1.0169e+01, 7.5872e+00, 5.8987e+00,\n",
      "        3.6665e+00, 4.1431e+00, 6.4864e+00, 2.7199e+00, 3.4642e+00, 3.0790e+00,\n",
      "        7.7583e+00, 5.8866e+00, 6.5362e+00, 3.2504e+00, 9.2173e+00, 3.6585e+00,\n",
      "        3.1988e+00, 8.5221e+00, 8.2731e+00, 3.8532e+00, 4.8022e+00, 3.6439e+00,\n",
      "        1.0935e+01, 8.0311e+00, 7.7913e+00, 4.5402e+00, 8.1012e+00, 7.9212e+00,\n",
      "        3.8407e+00, 3.7335e+00, 4.9638e+00, 4.2604e+00, 3.6005e+00, 2.8144e+00,\n",
      "        8.0206e+00, 6.4651e+00, 8.3159e+00, 1.0080e+01, 7.8743e+00, 4.0691e+00,\n",
      "        1.1740e+01, 7.9402e+00, 8.9261e+00, 5.4996e+00, 7.3240e+00, 6.5072e+00,\n",
      "        6.8937e+00, 6.4240e+00, 6.0351e+00, 6.5420e+00, 3.3728e+00, 3.4072e+00,\n",
      "        7.8001e+00, 1.0342e+01, 8.0005e+00, 7.6132e+00, 4.4378e+00, 7.4117e+00,\n",
      "        3.7099e+00, 8.4993e+00, 3.3434e+00, 6.3167e+00, 8.9073e+00, 1.0760e+01,\n",
      "        3.3704e+00, 8.0722e+00, 9.9143e+00, 2.9858e+00, 1.1054e+01, 7.2226e+00,\n",
      "        4.0236e+00, 7.7638e+00, 8.8652e+00, 4.8147e+00, 8.3356e+00, 6.1913e+00,\n",
      "        5.5323e+00, 5.3074e+00, 1.0635e+01, 8.3874e+00, 4.3197e+00, 7.3936e+00,\n",
      "        3.8904e+00, 7.7307e+00, 5.5206e+00, 6.8335e+00, 8.7278e+00, 8.7923e+00,\n",
      "        7.5482e+00, 6.3315e+00, 7.6907e+00, 6.5191e+00, 8.1112e+00, 6.0744e+00,\n",
      "        7.6293e+00, 3.2339e+00, 3.6320e+00, 7.3206e+00, 7.3969e+00, 5.5635e+00,\n",
      "        3.7294e+00, 6.8773e+00, 6.9312e+00, 3.9753e+00, 6.1324e+00, 4.2676e+00,\n",
      "        8.9605e+00, 8.6493e+00, 4.2889e+00, 9.2434e+00, 4.2076e+00, 6.8369e+00,\n",
      "        7.5808e+00, 5.8026e+00, 6.3999e+00, 8.6655e+00, 4.5546e+00, 7.5173e+00,\n",
      "        3.6579e+00, 3.8929e+00, 3.8135e+00, 7.0275e+00, 6.7500e+00, 6.1509e+00,\n",
      "        4.6282e+00, 5.6616e+00, 3.1346e+00, 6.0289e+00, 6.8392e+00, 1.0248e+01,\n",
      "        4.8734e+00, 8.2616e+00, 8.2552e+00, 9.0626e+00, 5.1248e+00, 1.0442e+01,\n",
      "        4.3501e+00, 4.0452e+00, 8.0550e+00, 8.0169e+00, 5.5064e+00, 4.8750e+00,\n",
      "        3.2423e+00, 9.3010e+00, 9.4527e+00, 4.3195e+00, 5.8444e+00, 7.7259e+00,\n",
      "        6.9407e+00, 9.4284e+00, 6.8794e+00, 6.2777e+00, 4.9284e+00, 4.0363e+00,\n",
      "        5.2891e+00, 6.8253e+00, 3.4332e+00, 4.6554e+00, 9.6543e+00, 5.4675e+00,\n",
      "        7.0246e+00, 8.9768e+00, 7.3552e+00, 7.0843e+00, 4.0019e+00, 7.0167e+00,\n",
      "        5.3832e+00, 7.0198e+00, 6.2728e+00, 7.9760e+00, 6.7009e+00, 3.1008e+00,\n",
      "        3.7517e+00, 8.4069e+00, 5.8823e+00, 7.8409e+00, 6.1451e+00, 4.5688e+00,\n",
      "        8.5194e+00, 5.0232e+00, 7.1055e+00, 7.4458e+00, 6.5165e+00, 4.4818e+00,\n",
      "        3.0320e+00, 3.1900e+00, 4.4637e+00, 4.5335e+00, 9.5023e+00, 6.0199e+00,\n",
      "        5.4011e+00, 3.2444e+00, 4.2720e+00, 6.1354e+00, 6.2758e+00, 8.9967e+00,\n",
      "        4.5214e+00, 3.1573e+00, 6.0574e+00, 4.0403e+00, 7.8152e+00, 8.4156e+00,\n",
      "        4.6008e+00, 5.8804e+00, 7.1358e+00, 6.0200e+00, 7.3131e+00, 8.1553e+00,\n",
      "        1.3175e+01, 7.3574e+00, 5.9725e+00, 7.8287e+00, 8.7458e+00, 3.1737e+00,\n",
      "        9.2534e+00, 4.9216e+00, 3.6631e+00, 1.0356e+01, 5.2272e+00, 5.3937e+00,\n",
      "        3.6740e+00, 5.0459e+00, 5.8647e+00, 2.9649e+00, 9.9867e+00, 8.0226e+00,\n",
      "        9.2221e+00, 8.9030e+00, 5.4759e+00, 6.5469e+00, 5.1895e+00, 7.2515e+00,\n",
      "        6.1461e+00, 9.4489e+00, 7.6987e+00, 4.7674e+00, 5.1444e+00, 6.8019e+00,\n",
      "        3.2297e+00, 4.3355e+00, 7.1683e+00, 9.6764e+00, 6.3787e+00, 6.8448e+00,\n",
      "        7.7832e+00, 4.1560e+00, 6.7557e+00, 4.0458e+00, 5.6040e+00, 7.9996e+00,\n",
      "        9.0161e+00, 4.8488e+00, 7.1434e+00, 1.0233e+01, 7.9647e+00, 7.0791e+00,\n",
      "        7.6245e+00, 5.8458e+00, 8.1646e+00, 3.7692e+00, 2.7783e+00, 9.1817e+00,\n",
      "        8.9870e+00, 5.7620e+00, 7.6733e+00, 9.4394e+00, 5.4063e+00, 6.1194e+00,\n",
      "        7.5908e+00, 7.9801e+00, 4.0513e+00, 1.1060e+01, 5.4489e+00, 4.8743e+00,\n",
      "        7.7133e+00, 5.4406e+00, 4.8402e+00, 8.9713e+00, 8.4065e+00, 6.4486e+00,\n",
      "        5.4452e+00, 5.0005e+00, 6.3390e+00, 6.6256e+00, 4.5155e+00, 1.1934e+01,\n",
      "        6.2630e+00, 7.4090e+00, 5.7174e+00, 8.5916e+00, 3.6143e+00, 4.0982e+00,\n",
      "        7.5280e+00, 8.0798e+00, 9.3423e+00, 1.0322e+01, 7.9871e+00, 7.7879e+00,\n",
      "        9.1359e+00, 9.9205e+00, 3.2464e+00, 1.0149e+01, 3.5187e+00, 3.8942e+00,\n",
      "        3.3527e+00, 1.0046e+01, 6.9938e+00, 5.5918e+00, 5.8392e+00, 5.8840e+00,\n",
      "        5.2717e+00, 7.1250e+00, 6.4935e+00, 7.5266e+00, 4.8432e+00, 6.4840e+00,\n",
      "        5.9703e+00, 6.5785e+00, 4.1218e+00, 4.8969e+00, 8.6725e+00, 7.6008e+00,\n",
      "        3.4753e+00, 9.0548e+00, 9.5105e+00, 7.3130e+00, 7.5978e+00, 4.2357e+00,\n",
      "        3.1635e+00, 3.2217e+00, 3.8252e+00, 5.6020e+00, 8.9569e+00, 1.0037e+01,\n",
      "        7.5993e+00, 8.7379e+00, 6.1046e+00, 6.2674e+00, 4.6313e+00, 5.7645e+00,\n",
      "        8.6861e+00, 5.3649e+00, 6.3887e+00, 6.9564e+00, 8.4904e+00, 9.0691e+00,\n",
      "        8.0351e+00, 8.1551e+00, 1.0352e+01, 4.6277e+00, 6.4871e+00, 3.5759e+00,\n",
      "        7.1810e+00, 8.2247e+00, 8.3666e+00, 7.2797e+00, 8.3372e+00, 9.5250e+00,\n",
      "        4.2348e+00, 6.9679e+00, 3.3081e+00, 4.1617e+00, 8.8028e+00, 5.0941e+00,\n",
      "        8.0214e+00, 4.1189e+00, 9.2540e+00, 6.0210e+00, 1.0214e+01, 6.2476e+00,\n",
      "        6.6060e+00, 3.3495e+00, 6.6210e+00, 8.5403e+00, 4.7837e+00, 6.5657e+00,\n",
      "        8.7610e+00, 1.0930e+01, 5.6443e+00, 4.2376e+00, 6.7275e+00, 7.2797e+00,\n",
      "        5.5411e+00, 3.4101e+00, 9.1764e+00, 3.3827e+00, 7.0469e+00, 9.7554e+00,\n",
      "        8.8867e+00, 5.8164e+00, 4.6305e+00, 3.2593e+00, 6.9429e+00, 7.8459e+00,\n",
      "        5.7315e+00, 3.7180e+00, 6.6571e+00, 7.0325e+00, 5.4102e+00, 7.7311e+00,\n",
      "        9.2592e+00, 7.9585e+00, 5.7901e+00, 4.9911e+00, 8.0818e+00, 3.9442e+00,\n",
      "        5.6843e+00, 8.1992e+00, 5.4445e+00, 9.1855e+00, 7.3715e+00, 9.6595e+00,\n",
      "        8.2389e+00, 6.1789e+00, 5.3306e+00, 3.1707e+00, 6.6046e+00, 4.1405e+00,\n",
      "        6.1913e+00, 9.8724e+00, 1.1903e+01, 7.4566e+00, 3.2879e+00, 6.6584e+00,\n",
      "        7.2274e+00, 7.7887e+00, 6.3385e+00, 3.9408e+00, 7.7398e+00, 8.2514e+00,\n",
      "        3.1091e+00, 4.6117e+00, 4.3439e+00, 5.4314e+00, 1.0196e+01, 3.1085e+00,\n",
      "        8.2253e+00, 8.9730e+00, 5.5733e+00, 7.3355e+00, 3.9841e+00, 4.3511e+00,\n",
      "        9.4545e+00, 8.9525e+00, 7.9818e+00, 5.5268e+00, 3.3277e+00, 6.8544e+00,\n",
      "        3.4204e+00, 4.7237e+00, 6.6715e+00, 4.1715e+00, 1.0536e+01, 3.8278e+00,\n",
      "        3.0783e+00, 8.4159e+00, 5.0820e+00, 3.6295e+00, 3.9721e+00, 4.2813e+00,\n",
      "        5.5505e+00, 6.5014e+00, 5.1746e+00, 4.0295e+00, 7.1305e+00, 3.4740e+00,\n",
      "        9.3149e+00, 5.5529e+00, 7.5499e+00, 9.9039e+00, 1.0492e+01, 7.0586e+00,\n",
      "        8.3753e+00, 5.6708e+00, 5.7051e+00, 3.6620e+00, 3.4946e+00, 7.0924e+00,\n",
      "        3.4594e+00, 3.4254e+00, 6.1138e+00, 6.2502e+00, 7.4222e+00, 6.7098e+00,\n",
      "        6.5851e+00, 5.4311e+00, 4.9711e+00, 5.8090e+00, 9.7436e+00, 4.5645e+00,\n",
      "        9.0597e+00, 6.2629e+00, 8.6934e+00, 5.3439e+00, 6.6585e+00, 6.2897e+00,\n",
      "        3.9618e+00, 8.2322e+00, 3.7826e+00, 7.6410e+00, 7.1041e+00, 3.1472e+00,\n",
      "        1.0193e+01, 6.9154e+00, 5.1198e+00, 9.6668e+00, 7.7261e+00, 1.0099e+01,\n",
      "        3.5088e+00, 3.2995e+00, 4.2222e+00, 8.1879e+00, 6.9679e+00, 4.1124e+00,\n",
      "        8.0338e+00, 7.2612e+00, 5.4664e+00, 3.6382e+00, 1.1596e+01, 3.2388e+00,\n",
      "        6.8149e+00, 4.9996e+00, 3.9202e+00, 8.6017e+00, 8.0561e+00, 7.5113e+00,\n",
      "        5.8668e+00, 5.3181e+00, 6.5807e+00, 7.3360e+00, 7.6761e+00, 5.0264e+00,\n",
      "        7.5148e+00, 4.1214e+00, 4.8010e+00, 3.3535e+00, 6.7379e+00, 6.5749e+00,\n",
      "        9.3369e+00, 7.5056e+00, 3.9662e+00, 7.5930e+00, 8.5468e+00, 6.6547e+00,\n",
      "        4.1656e+00, 3.5718e+00, 6.6771e+00, 6.2141e+00, 6.7554e+00, 4.2915e+00,\n",
      "        4.3730e+00, 4.7732e+00, 3.6596e+00, 5.1667e+00, 3.7722e+00, 5.9924e+00,\n",
      "        6.6696e+00, 1.0077e+01, 7.3898e+00, 1.0620e+01, 5.1769e+00, 6.4129e+00,\n",
      "        6.6174e+00, 4.5682e+00, 6.8032e+00, 3.7621e+00, 4.1933e+00, 6.2957e+00,\n",
      "        6.3181e+00, 3.5080e+00, 7.1979e+00, 9.2227e+00, 1.0598e+01, 7.1854e+00,\n",
      "        7.6132e+00, 5.6203e+00, 3.9545e+00, 3.4678e+00, 8.3148e+00, 6.6055e+00,\n",
      "        5.7044e+00, 6.5530e+00, 4.2763e+00, 4.5564e+00, 8.1213e+00, 3.7306e+00,\n",
      "        7.6306e+00, 6.2797e+00, 3.1790e+00, 4.1180e+00, 8.9480e+00, 9.6137e+00,\n",
      "        8.8492e+00, 4.9181e+00, 8.4994e+00, 4.0183e+00, 2.9590e+00, 9.1080e+00,\n",
      "        7.7125e+00, 6.9215e+00, 4.3087e+00, 6.6301e+00, 4.1398e+00, 4.4333e+00,\n",
      "        5.4009e+00, 4.8327e+00, 7.2381e+00, 9.7532e+00, 6.6051e+00, 9.4689e+00,\n",
      "        8.9048e+00, 4.8537e+00, 3.8479e+00, 3.4448e+00, 6.1351e+00, 6.9675e+00,\n",
      "        4.0607e+00, 9.0515e+00, 6.8805e+00, 3.9059e+00, 7.9803e+00, 8.4326e+00,\n",
      "        4.2576e+00, 9.6461e+00, 3.8228e+00, 5.9161e+00, 4.3125e+00, 8.7515e+00,\n",
      "        6.7558e+00, 4.4441e+00, 7.7946e+00, 7.2377e+00, 5.2940e+00, 5.9425e+00,\n",
      "        8.2093e+00, 8.8801e+00, 5.3729e+00, 4.5821e+00, 6.3753e+00, 4.8333e+00,\n",
      "        3.4730e+00, 7.6036e+00, 3.1603e+00, 6.7631e+00, 8.3287e+00, 5.2993e+00,\n",
      "        4.3582e+00, 6.0527e+00, 6.3237e+00, 3.5270e+00, 3.8721e+00, 6.8188e+00,\n",
      "        8.4933e+00, 9.1868e+00, 9.9125e+00, 9.6358e+00, 4.9809e+00, 7.0806e+00,\n",
      "        4.6030e+00, 7.5390e+00, 3.9722e+00, 8.2720e+00, 7.4056e+00, 4.9248e+00,\n",
      "        7.9225e+00, 7.1746e+00, 5.7956e+00, 5.6589e+00, 4.7550e+00, 5.0150e+00,\n",
      "        4.2270e+00, 5.1273e+00, 6.7738e+00, 4.3100e+00, 3.7132e+00, 5.5645e+00,\n",
      "        6.4610e+00, 6.1236e+00, 5.0087e+00, 4.2809e+00, 6.6285e+00, 4.3374e+00,\n",
      "        8.3437e+00, 4.5377e+00, 1.0937e+01, 5.5934e+00, 6.6276e+00, 6.7316e+00,\n",
      "        6.0331e+00, 3.5179e+00, 4.8322e+00, 8.9519e+00, 7.6807e+00, 4.6090e+00,\n",
      "        7.2687e+00, 4.6926e+00, 3.5030e+00, 3.6694e+00, 4.4121e+00, 8.7641e+00,\n",
      "        4.1147e+00, 7.2446e+00, 6.9879e+00, 7.9971e+00, 8.1946e+00, 8.4622e+00,\n",
      "        8.7818e+00, 3.2322e+00, 6.2307e+00, 9.7983e+00, 6.3199e+00, 7.8564e+00,\n",
      "        4.1819e+00, 8.3454e+00, 5.5920e+00, 6.9589e+00, 8.8104e+00, 6.6126e+00,\n",
      "        6.9771e+00, 3.9985e+00, 6.6719e+00, 8.5807e+00, 8.3777e+00, 4.3859e+00,\n",
      "        3.4562e+00, 7.3605e+00, 6.5123e+00, 8.0937e+00, 3.4849e+00, 5.4233e+00,\n",
      "        6.5565e+00, 6.7040e+00, 3.0468e+00, 9.3260e+00, 5.7090e+00, 6.2299e+00,\n",
      "        7.2641e+00, 8.8330e+00, 6.6351e+00, 1.0536e+01, 6.3033e+00, 6.6124e+00,\n",
      "        7.8870e+00, 5.9390e+00, 5.5359e+00, 6.0763e+00, 3.7156e+00, 7.2448e+00,\n",
      "        3.4011e+00, 3.0382e+00, 6.1433e+00, 4.2324e+00, 3.6620e+00, 8.8876e+00,\n",
      "        4.9342e+00, 3.8828e+00, 9.5364e+00, 5.0030e+00, 3.6228e+00, 3.4539e+00,\n",
      "        8.4862e+00, 1.1764e+01, 5.7633e+00, 6.7112e+00, 6.6938e+00, 3.8852e+00,\n",
      "        8.2156e+00, 1.1442e+01, 3.6966e+00, 6.5671e+00, 9.1061e+00, 9.9237e+00,\n",
      "        6.8176e+00, 3.6183e+00, 6.8239e+00, 7.0038e+00, 2.9041e+00, 3.3114e+00,\n",
      "        1.0519e+01, 7.9886e+00, 6.4125e+00, 4.7667e+00, 5.0001e+00, 8.5394e+00,\n",
      "        7.0942e+00, 4.7757e+00, 1.0487e+01, 7.7159e+00, 6.6101e+00, 6.2488e+00,\n",
      "        8.4627e+00, 9.6524e+00, 6.0289e+00, 3.4751e+00, 6.9749e+00, 6.2346e+00,\n",
      "        1.2663e+01, 3.9103e+00, 7.7273e+00, 6.1698e+00])\n",
      "torch.Size([20000])\n",
      "torch.Size([4803, 910])\n",
      "tensor([1.0000e+01, 2.0612e+05, 7.8009e+00, 3.7240e+00, 6.3249e+00, 5.1240e+00,\n",
      "        4.5632e+00, 3.4379e+00, 6.9096e+00, 6.5689e+00, 5.0217e+00, 6.7563e+00,\n",
      "        5.6691e+00, 5.6659e+00, 8.4679e+00, 6.4551e+00, 3.2712e+00, 4.1205e+00,\n",
      "        6.2173e+00, 8.7078e+00, 8.1299e+00, 7.8890e+00, 8.4005e+00, 3.6599e+00,\n",
      "        9.4834e+00, 4.4335e+00, 4.8298e+00, 6.0747e+00, 6.3031e+00, 2.7472e+00,\n",
      "        6.6321e+00, 6.1678e+00, 3.8487e+00, 4.5912e+00, 8.4895e+00, 7.8759e+00,\n",
      "        3.3507e+00, 5.3877e+00, 7.3334e+00, 5.3274e+00, 7.7410e+00, 6.3260e+00,\n",
      "        7.4494e+00, 7.1535e+00, 7.6918e+00, 6.0934e+00, 7.1334e+00, 8.7939e+00,\n",
      "        8.6024e+00, 9.2792e+00, 8.1910e+00, 4.6708e+00, 5.1932e+00, 2.8254e+00,\n",
      "        4.7235e+00, 7.4260e+00, 6.2113e+00, 8.8296e+00, 5.8215e+00, 8.3595e+00,\n",
      "        8.3757e+00, 8.7658e+00, 4.5260e+00, 5.8730e+00, 4.6855e+00, 9.0200e+00,\n",
      "        3.3144e+00, 5.9196e+00, 5.4307e+00, 3.2591e+00, 6.7867e+00, 4.8180e+00,\n",
      "        5.1602e+00, 7.9481e+00, 5.9165e+00, 3.4731e+00, 4.1421e+00, 8.6205e+00,\n",
      "        5.1499e+00, 5.5405e+00, 3.3878e+00, 9.5085e+00, 6.4242e+00, 6.9933e+00,\n",
      "        3.1255e+00, 7.7767e+00, 4.7066e+00, 6.0572e+00, 7.2762e+00, 8.3661e+00,\n",
      "        4.3762e+00, 9.9737e+00, 9.9742e+00, 6.1779e+00, 6.4201e+00, 6.1817e+00,\n",
      "        5.0143e+00, 5.0942e+00, 5.3305e+00, 6.6589e+00, 5.1706e+00, 7.7564e+00,\n",
      "        3.2613e+00, 7.2941e+00, 4.3645e+00, 4.4709e+00, 7.4454e+00, 6.7203e+00,\n",
      "        3.3760e+00, 3.1099e+00, 4.1277e+00, 9.3790e+00, 9.2048e+00, 5.2737e+00,\n",
      "        4.3173e+00, 4.4473e+00, 6.5434e+00, 2.6640e+00, 8.6981e+00, 3.4893e+00,\n",
      "        7.4455e+00, 6.7293e+00, 3.1520e+00, 3.0604e+00, 9.1342e+00, 4.0014e+00,\n",
      "        4.4306e+00, 9.0742e+00, 8.2818e+00, 2.9562e+00, 4.9606e+00, 3.5175e+00,\n",
      "        8.8809e+00, 8.5991e+00, 8.3274e+00, 7.8068e+00, 6.5790e+00, 7.3735e+00,\n",
      "        6.1163e+00, 3.8292e+00, 8.5754e+00, 1.0168e+01, 3.0492e+00, 2.9248e+00,\n",
      "        8.3591e+00, 9.9236e+00, 7.5502e+00, 9.6090e+00, 3.4143e+00, 4.0050e+00,\n",
      "        1.2414e+01, 1.0650e+01, 7.9152e+00, 4.9462e+00, 7.8795e+00, 6.9918e+00,\n",
      "        7.1379e+00, 3.5450e+00, 5.7742e+00, 7.7251e+00, 3.3402e+00, 1.1473e+01,\n",
      "        7.1699e+00, 9.2413e+00, 7.4799e+00, 8.5001e+00, 4.2536e+00, 7.2524e+00,\n",
      "        4.2310e+00, 6.5540e+00, 3.7857e+00, 5.5025e+00, 8.4231e+00, 9.9447e+00,\n",
      "        3.3456e+00, 8.9274e+00, 1.0031e+01, 3.0759e+00, 9.4045e+00, 4.5084e+00,\n",
      "        4.5364e+00, 5.7961e+00, 9.8656e+00, 4.4939e+00, 8.0455e+00, 6.1519e+00,\n",
      "        4.0535e+00, 4.6625e+00, 8.2121e+00, 8.1151e+00, 5.1818e+00, 5.1381e+00,\n",
      "        3.1766e+00, 5.4309e+00, 1.0135e+01, 5.9766e+00, 8.1056e+00, 8.5687e+00,\n",
      "        5.8695e+00, 7.1038e+00, 7.6071e+00, 5.9889e+00, 7.3915e+00, 5.8034e+00,\n",
      "        8.4723e+00, 3.4284e+00, 7.9742e+00, 5.1605e+00, 7.7074e+00, 6.1427e+00,\n",
      "        5.3127e+00, 4.5849e+00, 7.4087e+00, 5.3649e+00, 7.4666e+00, 4.7867e+00,\n",
      "        9.3433e+00, 9.2357e+00, 4.1313e+00, 1.0701e+01, 4.8156e+00, 6.7085e+00,\n",
      "        3.6782e+00, 5.9073e+00, 6.2012e+00, 8.4294e+00, 4.8909e+00, 7.7243e+00,\n",
      "        4.6288e+00, 4.3086e+00, 3.7285e+00, 6.2819e+00, 8.5632e+00, 5.5545e+00,\n",
      "        4.6561e+00, 5.6161e+00, 3.7751e+00, 4.9640e+00, 7.2335e+00, 9.7481e+00,\n",
      "        5.2133e+00, 7.6015e+00, 8.6895e+00, 1.0113e+01, 6.0616e+00, 1.0018e+01,\n",
      "        4.1844e+00, 6.1055e+00, 8.6274e+00, 8.1236e+00, 4.7989e+00, 4.8268e+00,\n",
      "        3.9244e+00, 1.0487e+01, 8.1472e+00, 4.7222e+00, 5.4460e+00, 4.9714e+00,\n",
      "        7.9876e+00, 8.1681e+00, 7.1519e+00, 3.7090e+00, 6.5660e+00, 3.7652e+00,\n",
      "        5.2398e+00, 6.2852e+00, 5.5966e+00, 5.2405e+00, 8.1811e+00, 4.8230e+00,\n",
      "        3.9760e+00, 9.1819e+00, 6.8444e+00, 8.8278e+00, 4.6352e+00, 6.5241e+00,\n",
      "        4.3096e+00, 6.9998e+00, 5.4018e+00, 7.9002e+00, 7.9539e+00, 3.3278e+00,\n",
      "        4.5453e+00, 1.0769e+01, 6.2151e+00, 7.3360e+00, 5.8120e+00, 4.4893e+00,\n",
      "        7.2518e+00, 4.9242e+00, 6.9274e+00, 8.1527e+00, 6.4609e+00, 4.7944e+00,\n",
      "        2.9849e+00, 3.0822e+00, 5.9765e+00, 4.8508e+00, 8.0498e+00, 6.5124e+00,\n",
      "        5.1241e+00, 3.0443e+00, 3.4390e+00, 5.3484e+00, 6.2889e+00, 7.7185e+00,\n",
      "        4.7104e+00, 5.3860e+00, 5.6423e+00, 4.9156e+00, 8.5797e+00, 6.7357e+00,\n",
      "        6.0145e+00, 5.6018e+00, 8.4670e+00, 6.6227e+00, 6.9781e+00, 6.1687e+00,\n",
      "        1.2809e+01, 7.2808e+00, 4.5601e+00, 5.5845e+00, 6.4422e+00, 3.3062e+00,\n",
      "        7.9501e+00, 5.1862e+00, 3.8763e+00, 8.2615e+00, 3.5065e+00, 6.0301e+00,\n",
      "        4.1902e+00, 5.9942e+00, 4.5103e+00, 3.0136e+00, 5.9972e+00, 7.4001e+00,\n",
      "        7.1182e+00, 8.0512e+00, 7.6982e+00, 6.9870e+00, 6.7490e+00, 7.1420e+00,\n",
      "        6.5708e+00, 7.9691e+00, 6.8515e+00, 4.6919e+00, 6.5948e+00, 7.3749e+00,\n",
      "        3.1147e+00, 5.9227e+00, 6.4327e+00, 9.5803e+00, 8.4809e+00, 5.8472e+00,\n",
      "        7.1840e+00, 3.8311e+00, 8.1204e+00, 1.1070e+01, 6.7159e+00, 6.8480e+00,\n",
      "        8.6108e+00, 3.7807e+00, 5.9479e+00, 1.0716e+01, 7.7463e+00, 7.3647e+00,\n",
      "        7.1699e+00, 5.0901e+00, 7.3462e+00, 3.6515e+00, 2.6981e+00, 8.5194e+00,\n",
      "        8.5338e+00, 9.3580e+00, 5.7563e+00, 8.2532e+00, 5.1822e+00, 3.9531e+00,\n",
      "        7.4608e+00, 5.2114e+00, 4.1042e+00, 7.1538e+00, 3.5819e+00, 4.5922e+00,\n",
      "        7.5304e+00, 3.4401e+00, 4.8100e+00, 9.1442e+00, 7.3562e+00, 4.1055e+00,\n",
      "        5.9407e+00, 4.9256e+00, 5.9262e+00, 6.4814e+00, 8.1930e+00, 1.2555e+01,\n",
      "        6.2727e+00, 8.7331e+00, 6.0577e+00, 8.9714e+00, 3.0298e+00, 4.2991e+00,\n",
      "        7.3774e+00, 8.5198e+00, 9.2705e+00, 7.1550e+00, 6.4425e+00, 8.1602e+00,\n",
      "        8.9625e+00, 8.9699e+00, 4.8284e+00, 4.1480e+00, 4.3143e+00, 3.5711e+00,\n",
      "        3.4317e+00, 9.0950e+00, 6.3797e+00, 5.4769e+00, 6.3877e+00, 6.7138e+00,\n",
      "        2.9796e+00, 7.1618e+00, 4.4977e+00, 7.7691e+00, 8.8016e+00, 5.9256e+00,\n",
      "        4.3305e+00, 6.5208e+00, 5.6867e+00, 4.8247e+00, 8.7826e+00, 8.0651e+00,\n",
      "        4.0145e+00, 9.3454e+00, 9.9253e+00, 3.1867e+00, 7.8471e+00, 5.4575e+00,\n",
      "        3.2178e+00, 6.8359e+00, 3.8305e+00, 5.8617e+00, 9.1641e+00, 9.8017e+00,\n",
      "        7.2524e+00, 7.5814e+00, 5.5272e+00, 7.5340e+00, 3.6817e+00, 6.4985e+00,\n",
      "        8.4665e+00, 5.4764e+00, 9.8095e+00, 5.9687e+00, 8.0795e+00, 8.2259e+00,\n",
      "        9.5932e+00, 7.4445e+00, 8.3135e+00, 3.2932e+00, 6.1059e+00, 7.5045e+00,\n",
      "        7.6704e+00, 7.3116e+00, 8.7702e+00, 7.3899e+00, 7.6613e+00, 9.7583e+00,\n",
      "        5.8726e+00, 6.6820e+00, 4.5469e+00, 4.4852e+00, 6.3762e+00, 5.7404e+00,\n",
      "        6.6003e+00, 5.5928e+00, 9.9241e+00, 5.0926e+00, 9.0157e+00, 7.4659e+00,\n",
      "        6.7366e+00, 6.3793e+00, 6.2417e+00, 4.6374e+00, 4.2573e+00, 6.9024e+00,\n",
      "        3.0659e+00, 9.2217e+00, 5.6415e+00, 4.1003e+00, 5.4484e+00, 7.5226e+00,\n",
      "        4.7545e+00, 8.7748e+00, 1.0328e+01, 4.3714e+00, 6.1525e+00, 8.2400e+00,\n",
      "        1.0420e+01, 6.1039e+00, 4.0138e+00, 8.4163e+00, 4.6010e+00, 7.0095e+00,\n",
      "        6.5408e+00, 3.9444e+00, 7.1921e+00, 7.5777e+00, 6.2940e+00, 7.7810e+00,\n",
      "        8.7631e+00, 6.8361e+00, 7.2712e+00, 7.1758e+00, 6.8422e+00, 3.7613e+00,\n",
      "        5.5692e+00, 8.6907e+00, 6.1351e+00, 8.4505e+00, 7.6351e+00, 9.4397e+00,\n",
      "        8.2600e+00, 6.4145e+00, 4.0844e+00, 3.2131e+00, 5.6228e+00, 4.5147e+00,\n",
      "        6.1080e+00, 1.0779e+01, 1.2602e+01, 4.6258e+00, 4.2324e+00, 6.4366e+00,\n",
      "        1.0077e+01, 9.2455e+00, 5.5774e+00, 4.0526e+00, 1.1828e+01, 6.7847e+00,\n",
      "        3.1313e+00, 6.6030e+00, 4.7902e+00, 6.0905e+00, 9.5345e+00, 3.5753e+00,\n",
      "        5.1459e+00, 7.7452e+00, 2.9090e+00, 5.8965e+00, 4.0222e+00, 3.7832e+00,\n",
      "        8.5781e+00, 1.0289e+01, 7.3142e+00, 6.4136e+00, 4.0884e+00, 7.7214e+00,\n",
      "        3.5145e+00, 5.0063e+00, 6.7397e+00, 5.6813e+00, 1.0601e+01, 8.6917e+00,\n",
      "        3.3927e+00, 7.1275e+00, 5.4586e+00, 4.2672e+00, 5.0083e+00, 4.6023e+00,\n",
      "        5.9967e+00, 6.8016e+00, 1.0626e+01, 4.1259e+00, 8.3958e+00, 3.5359e+00,\n",
      "        6.9504e+00, 8.0755e+00, 6.9281e+00, 8.3999e+00, 1.1002e+01, 7.3028e+00,\n",
      "        8.0577e+00, 5.5686e+00, 5.2025e+00, 3.9419e+00, 4.9242e+00, 7.3654e+00,\n",
      "        3.5384e+00, 3.5968e+00, 6.7945e+00, 5.8149e+00, 6.2255e+00, 5.9361e+00,\n",
      "        7.4398e+00, 7.7893e+00, 4.3783e+00, 5.0475e+00, 8.3584e+00, 4.5676e+00,\n",
      "        7.5176e+00, 3.6968e+00, 6.4129e+00, 5.6437e+00, 4.9334e+00, 9.0062e+00,\n",
      "        4.3517e+00, 8.0373e+00, 3.2798e+00, 3.3804e+00, 6.3989e+00, 4.0362e+00,\n",
      "        2.9499e+00, 6.8016e+00, 7.8761e+00, 7.7945e+00, 7.4169e+00, 8.8195e+00,\n",
      "        3.8383e+00, 3.2530e+00, 4.3921e+00, 8.5322e+00, 6.3236e+00, 3.6026e+00,\n",
      "        7.4595e+00, 5.6187e+00, 5.5307e+00, 3.7973e+00, 1.1329e+01, 4.0294e+00,\n",
      "        8.6561e+00, 7.9966e+00, 4.3640e+00, 7.6920e+00, 5.5098e+00, 8.0078e+00,\n",
      "        5.4144e+00, 5.7082e+00, 7.9979e+00, 7.3363e+00, 8.6475e+00, 5.0638e+00,\n",
      "        7.0478e+00, 4.0275e+00, 4.8653e+00, 4.3229e+00, 6.8212e+00, 3.6236e+00,\n",
      "        8.8069e+00, 6.2007e+00, 4.1492e+00, 7.7962e+00, 6.3304e+00, 7.9210e+00,\n",
      "        4.6274e+00, 3.7896e+00, 3.9081e+00, 5.4755e+00, 5.2198e+00, 3.4238e+00,\n",
      "        3.0021e+00, 5.2229e+00, 4.1073e+00, 5.1299e+00, 3.6923e+00, 5.4300e+00,\n",
      "        3.8069e+00, 7.9398e+00, 6.0380e+00, 1.0289e+01, 5.3053e+00, 5.1526e+00,\n",
      "        7.1941e+00, 4.3794e+00, 7.8569e+00, 4.7395e+00, 4.4693e+00, 8.2827e+00,\n",
      "        4.1860e+00, 3.6635e+00, 6.9619e+00, 8.8312e+00, 9.3977e+00, 4.4791e+00,\n",
      "        7.7738e+00, 4.7302e+00, 3.8846e+00, 3.4562e+00, 8.1774e+00, 8.4291e+00,\n",
      "        5.1453e+00, 3.2376e+00, 3.9506e+00, 5.9088e+00, 7.7994e+00, 4.5533e+00,\n",
      "        6.4407e+00, 3.7782e+00, 3.4583e+00, 5.4410e+00, 7.6706e+00, 9.1475e+00,\n",
      "        5.2521e+00, 4.9128e+00, 7.4838e+00, 3.4282e+00, 3.1306e+00, 9.2369e+00,\n",
      "        6.4222e+00, 6.4082e+00, 3.7306e+00, 8.1643e+00, 3.4905e+00, 4.6034e+00,\n",
      "        5.1364e+00, 3.0948e+00, 7.4398e+00, 9.2708e+00, 6.3760e+00, 9.3684e+00,\n",
      "        9.5882e+00, 4.4222e+00, 4.3584e+00, 4.0196e+00, 6.1301e+00, 6.4101e+00,\n",
      "        4.0953e+00, 9.4590e+00, 6.5843e+00, 4.4878e+00, 7.4940e+00, 7.8135e+00,\n",
      "        4.6054e+00, 9.5297e+00, 3.5717e+00, 5.5984e+00, 6.2287e+00, 3.8483e+00,\n",
      "        8.0074e+00, 6.5140e+00, 8.9473e+00, 6.9609e+00, 5.2560e+00, 6.2657e+00,\n",
      "        7.2773e+00, 1.0631e+01, 4.1071e+00, 3.0136e+00, 6.9195e+00, 6.2177e+00,\n",
      "        9.8317e+00, 8.6893e+00, 6.8916e+00, 7.3164e+00, 6.1229e+00, 4.2465e+00,\n",
      "        3.7476e+00, 5.9882e+00, 6.3554e+00, 3.4898e+00, 4.5076e+00, 9.7125e+00,\n",
      "        6.9453e+00, 7.3992e+00, 8.0923e+00, 8.2878e+00, 3.9605e+00, 7.0763e+00,\n",
      "        4.4184e+00, 9.1895e+00, 6.7347e+00, 8.0593e+00, 7.7333e+00, 4.8508e+00,\n",
      "        6.7659e+00, 5.3325e+00, 6.1139e+00, 5.7224e+00, 5.5891e+00, 7.9100e+00,\n",
      "        4.0054e+00, 7.6015e+00, 5.3563e+00, 3.9630e+00, 3.8441e+00, 5.6634e+00,\n",
      "        5.8812e+00, 6.4176e+00, 3.9290e+00, 4.7082e+00, 7.3301e+00, 4.2413e+00,\n",
      "        6.3378e+00, 3.6728e+00, 1.0002e+01, 6.0057e+00, 3.6497e+00, 7.2595e+00,\n",
      "        7.1271e+00, 3.8942e+00, 5.0902e+00, 8.2031e+00, 7.2538e+00, 4.5045e+00,\n",
      "        9.4012e+00, 5.0520e+00, 3.5241e+00, 4.0542e+00, 4.1224e+00, 6.7533e+00,\n",
      "        7.3016e+00, 6.8163e+00, 3.9306e+00, 7.9762e+00, 6.9420e+00, 7.3766e+00,\n",
      "        9.0998e+00, 3.3860e+00, 6.4105e+00, 9.0185e+00, 6.4955e+00, 7.3421e+00,\n",
      "        4.4762e+00, 7.7250e+00, 6.4793e+00, 5.9602e+00, 7.2460e+00, 4.0595e+00,\n",
      "        6.4430e+00, 4.5108e+00, 6.6240e+00, 7.8520e+00, 9.0445e+00, 4.3708e+00,\n",
      "        5.5103e+00, 6.5905e+00, 5.9591e+00, 7.1985e+00, 3.5282e+00, 8.8702e+00,\n",
      "        5.1255e+00, 3.4643e+00, 3.0517e+00, 6.1805e+00, 6.2617e+00, 6.9306e+00,\n",
      "        7.2009e+00, 6.9070e+00, 6.5036e+00, 9.5585e+00, 6.7101e+00, 1.0429e+01,\n",
      "        6.3493e+00, 1.0614e+01, 5.1683e+00, 7.2251e+00, 5.0565e+00, 7.0219e+00,\n",
      "        3.6582e+00, 3.4787e+00, 7.6105e+00, 4.5473e+00, 3.3884e+00, 9.1297e+00,\n",
      "        5.1025e+00, 4.3709e+00, 9.3493e+00, 5.9419e+00, 4.2156e+00, 3.7389e+00,\n",
      "        7.9702e+00, 9.4012e+00, 7.8495e+00, 6.4125e+00, 7.4713e+00, 4.0284e+00,\n",
      "        7.0833e+00, 8.4729e+00, 3.8786e+00, 6.9943e+00, 8.1596e+00, 8.1142e+00,\n",
      "        7.6824e+00, 4.9672e+00, 7.1598e+00, 8.7082e+00, 2.9380e+00, 3.3577e+00,\n",
      "        1.1094e+01, 7.7726e+00, 6.6283e+00, 7.1819e+00, 6.1022e+00, 8.7592e+00,\n",
      "        5.2177e+00, 5.2246e+00, 7.4683e+00, 7.3587e+00, 6.9664e+00, 6.7725e+00,\n",
      "        6.8392e+00, 9.0341e+00, 6.3119e+00, 4.0426e+00, 9.5279e+00, 6.7461e+00,\n",
      "        1.1998e+01, 4.6102e+00, 7.9238e+00, 5.9679e+00])\n",
      "torch.Size([4803])\n"
     ]
    }
   ],
   "source": [
    "for i, (X_test_batch, y_test_batch) in enumerate(test_loader):  \n",
    "    print(X_test_batch.size())\n",
    "    print(X_test_batch[i])\n",
    "    print(y_test_batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y : tensor([[-5.4265],\n",
      "        [-5.1892],\n",
      "        [-5.0348],\n",
      "        ...,\n",
      "        [-5.0200],\n",
      "        [-5.2369],\n",
      "        [-5.4340]])\n",
      "\n",
      "\n",
      "predicted y : tensor([[ 6.3562],\n",
      "        [-5.2237],\n",
      "        [ 5.9712],\n",
      "        ...,\n",
      "        [-5.2248],\n",
      "        [-5.5852],\n",
      "        [-4.9042]])\n",
      "\n",
      "\n",
      "predicted y : tensor([[-5.5979],\n",
      "        [-5.5308],\n",
      "        [-5.2134],\n",
      "        ...,\n",
      "        [-5.1075],\n",
      "        [-4.8669],\n",
      "        [-3.0742]])\n",
      "\n",
      "\n",
      "predicted y : tensor([[-5.0457],\n",
      "        [-4.7312],\n",
      "        [-0.0742],\n",
      "        ...,\n",
      "        [-5.5227],\n",
      "        [-5.4729],\n",
      "        [-5.0668]])\n",
      "\n",
      "\n",
      "predicted y : tensor([[ 1.6689],\n",
      "        [-5.1567],\n",
      "        [-5.0377],\n",
      "        ...,\n",
      "        [18.5898],\n",
      "        [-4.6894],\n",
      "        [-5.3793]])\n",
      "\n",
      "\n",
      "84803\n",
      "tensor(1.1272e+10)\n",
      "Mean absolute difference of the network on the 84803 test values: 132924.687500\n"
     ]
    }
   ],
   "source": [
    "from numpy import vstack\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# ---------------------------------\n",
    "# Test the model\n",
    "# ---------------------------------\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    predictions, actuals = list(), list()\n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(X)\n",
    "        predicted = outputs.data\n",
    "\n",
    "        y_preds  = outputs.detach().numpy()\n",
    "        y_actual = y.detach().numpy() \n",
    "        y_actual = y_actual.reshape((len(y_actual), 1))\n",
    "\n",
    "        assert y_preds.shape == y_actual.shape\n",
    "        \n",
    "        predictions.append(y_preds)\n",
    "        actuals.append(y_actual)\n",
    "\n",
    "        total += y.size(0)\n",
    "        print(f\"predicted y : {predicted}\\n\\n\")\n",
    "        # print(f\"actual    y : {y}\\n\\n\")\n",
    "        correct += np.abs(predicted - y).sum()\n",
    "\n",
    "    print(total)\n",
    "    print(correct)\n",
    "    print(f\"Mean absolute difference of the network on the {len(test_loader.dataset.indices)} test values: {correct / total:2.6f}\")\n",
    "\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(actuals, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            input_size  = 910\n",
      "            hidden_size = 64\n",
      "            num_classes = 1\n",
      "        \n",
      "SimpleNN_GeneExpr(\n",
      "  (fc1): Linear(in_features=910, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Model structure: SimpleNN_GeneExpr(\n",
      "  (fc1): Linear(in_features=910, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Layer: fc1.weight | Size: torch.Size([64, 910]) | Values : tensor([[ 0.0031, -0.0272,  0.0308,  ..., -0.0007, -0.0200,  0.0291],\n",
      "        [ 0.0129, -0.0039, -0.0077,  ..., -0.0066, -0.0268,  0.0217]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc1.bias | Size: torch.Size([64]) | Values : tensor([-0.0063,  0.0170], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc2.weight | Size: torch.Size([1, 64]) | Values : tensor([[-0.1118, -0.0632, -0.0409, -0.0645, -0.0033,  0.0511, -0.0150,  0.0214,\n",
      "         -0.1160,  0.0529,  0.0969, -0.1158, -0.1116, -0.0962,  0.1024,  0.0488,\n",
      "         -0.1139, -0.1227,  0.0618,  0.0701,  0.0309,  0.0626,  0.0922, -0.0660,\n",
      "          0.0247,  0.1061,  0.0109, -0.0806, -0.0197,  0.0894, -0.0646, -0.1006,\n",
      "          0.0749, -0.1185,  0.0140,  0.0624,  0.0757, -0.0718,  0.0584, -0.0183,\n",
      "          0.0144,  0.0421,  0.0105, -0.1144, -0.0700,  0.1185,  0.0574,  0.1109,\n",
      "          0.0498,  0.0847, -0.0535, -0.0890, -0.0427,  0.0084, -0.1044, -0.0109,\n",
      "         -0.1012,  0.0138,  0.0625,  0.0934,  0.0506, -0.0069, -0.1076, -0.1036]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc2.bias | Size: torch.Size([1]) | Values : tensor([0.0545], grad_fn=<SliceBackward0>) \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([[129.3490],\n",
      "        [209.9591],\n",
      "        [  4.5756],\n",
      "        ...,\n",
      "        [621.0994],\n",
      "        [ 54.3161],\n",
      "        [922.4257]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([25000])) that is different to the input size (torch.Size([25000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.8719e+01],\n",
      "        [-9.6946e+02],\n",
      "        [-5.3419e+01],\n",
      "        ...,\n",
      "        [-4.1394e+05],\n",
      "        [-3.6030e+03],\n",
      "        [-6.9712e+01]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -36.7172],\n",
      "        [ -20.7834],\n",
      "        [-304.9717],\n",
      "        ...,\n",
      "        [ -25.3333],\n",
      "        [ -21.4341],\n",
      "        [-491.2570]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 14.4568],\n",
      "        [392.7724],\n",
      "        [ 78.6831],\n",
      "        ...,\n",
      "        [ 65.4693],\n",
      "        [  5.7414],\n",
      "        [ 56.5482]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.3643e+05],\n",
      "        [4.7884e+04],\n",
      "        [8.0635e+02],\n",
      "        ...,\n",
      "        [1.8662e+01],\n",
      "        [1.9338e+02],\n",
      "        [1.2746e+03]], grad_fn=<AddmmBackward0>)\n",
      "Epoch [1/10], Step [5/14], Loss: 1421595136.0000\n",
      "tensor([[ 71.0781],\n",
      "        [175.5704],\n",
      "        [652.1816],\n",
      "        ...,\n",
      "        [ 29.0352],\n",
      "        [227.7199],\n",
      "        [151.1791]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.3534e+01],\n",
      "        [3.8229e+00],\n",
      "        [3.1521e+02],\n",
      "        ...,\n",
      "        [5.8455e+04],\n",
      "        [1.6432e+01],\n",
      "        [4.3748e+00]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cwoest/Documents/Academics/Data_Science_UP/master_thesis/material/GNN-material/06_create_base_dataset.ipynb Cell 45'\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cwoest/Documents/Academics/Data_Science_UP/master_thesis/material/GNN-material/06_create_base_dataset.ipynb#ch0000077?line=69'>70</a>\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cwoest/Documents/Academics/Data_Science_UP/master_thesis/material/GNN-material/06_create_base_dataset.ipynb#ch0000077?line=70'>71</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cwoest/Documents/Academics/Data_Science_UP/master_thesis/material/GNN-material/06_create_base_dataset.ipynb#ch0000077?line=71'>72</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cwoest/Documents/Academics/Data_Science_UP/master_thesis/material/GNN-material/06_create_base_dataset.ipynb#ch0000077?line=72'>73</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cwoest/Documents/Academics/Data_Science_UP/master_thesis/material/GNN-material/06_create_base_dataset.ipynb#ch0000077?line=74'>75</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# was % 100 before\u001b[39;00m\n",
      "File \u001b[0;32m/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///users/cwoest/Applications/anaconda3/envs/master-thesis-log/lib/python3.10/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# Build the Neural Netowrk\n",
    "# ---------------------------------\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class SimpleNN_GeneExpr(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN_GeneExpr, self).__init__()\n",
    "        print(f\"\"\"\n",
    "            input_size  = {input_size}\n",
    "            hidden_size = {hidden_size}\n",
    "            num_classes = {num_classes}\n",
    "        \"\"\")\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)   # module that applies a linear transformation on the input using its stored weights and biases.\n",
    "        self.relu = nn.ReLU()                           # Non-linear activations are what create the complex mappings between the model’s inputs and outputs. No introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN_GeneExpr(input_size, hidden_size=64, num_classes=num_classes).to(device)\n",
    "print(model)\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "print(100*\"-\")\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# ---------------------------------\n",
    "# Train the model\n",
    "# ---------------------------------\n",
    "loss_values = []\n",
    "total_step = len(train_loader)\n",
    "it = iter(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):  \n",
    "        #images = images.reshape(-1, 28*28).to(device)\n",
    "        #labels = labels.to(device)\n",
    "        # if i > 3: break\n",
    "\n",
    "        # Move tensors to the configured device\n",
    "        X = X_batch.to(device)  # All feature values\n",
    "        y = y_batch.to(device)  # The ln(IC50) values\n",
    "        # print(20*\"+\")\n",
    "        # print(f\"i={i}\")\n",
    "        # print(X.size())\n",
    "        # print(y.size())\n",
    "        # print(X)\n",
    "        # print(y)\n",
    "        assert not torch.isnan(X).any(), \"X has NaN in it\"\n",
    "        assert not torch.isnan(X).any(), \"y has NaN in it\"\n",
    "        # print(\"outputs...\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        print(outputs)\n",
    "        loss = loss_func(outputs, y_batch)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 5 == 0: # was % 100 before\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))    \n",
    "\n",
    "    loss_values.append(running_loss / len(gene_expr_dataset))                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c78b81650a0bd32063743affb6953ff71b1a0dba806fbca9e2db842718495748"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('master-thesis-log')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
