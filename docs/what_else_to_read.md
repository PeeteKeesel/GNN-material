
- [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
- [Adam Optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)

- The learning rate is a parameter that determines how much an updating step influences the current value of the weights. 
- While weight decay is an additional term in the weight update rule that causes the weights to exponentially decay to zero, if no other update is scheduled.